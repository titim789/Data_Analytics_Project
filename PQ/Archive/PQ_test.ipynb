{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9becae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.vq import kmeans2, vq\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4210c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PQ_train(vectors, M, k):\n",
    "    s = int(vectors.shape[1] / M)                      # Dimension (or length) of a segment.\n",
    "    codebook = np.empty((M, k, s), np.float32)         \n",
    "        \n",
    "    for m in range(M):\n",
    "        sub_vectors = vectors[:, m*s:(m+1)*s]          # Sub-vectors for segment m.\n",
    "        codebook[m], label = kmeans2(sub_vectors, k)   # Run k-means clustering for each segment.\n",
    "        \n",
    "    return codebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93ca1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PQ_encode(vectors, codebook):\n",
    "    M, k, s = codebook.shape\n",
    "    PQ_code = np.empty((vectors.shape[0], M), np.uint8)\n",
    "    \n",
    "    for m in range(M):\n",
    "        sub_vectors = vectors[:, m*s:(m+1)*s]           # Sub-vectors for segment m.\n",
    "        centroid_ids, _ = vq(sub_vectors, codebook[m])  # vq returns the nearest centroid Ids.\n",
    "        PQ_code[:, m] = centroid_ids                    # Assign centroid Ids to PQ_code.\n",
    "        \n",
    "    return PQ_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c4329f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PQ_search(query_vector, codebook, PQ_code):\n",
    "    M, k, s = codebook.shape\n",
    "    \n",
    "    distance_table = np.empty((M, k), np.float32)    # Shape is (M, k)    \n",
    "        \n",
    "    for m in range(M):\n",
    "        query_segment = query_vector[m*s:(m+1)*s]    # Query vector for segment m.\n",
    "        distance_table[m] = cdist([query_segment], codebook[m], \"sqeuclidean\")[0]\n",
    "    \n",
    "    N, M = PQ_code.shape\n",
    "    distance_table = distance_table.T               # Transpose the distance table to shape (k, M)\n",
    "    distances = np.zeros((N, )).astype(np.float32)\n",
    "\n",
    "    for n in range(N):                              # For each PQ Code, lookup the partial distances.\n",
    "        for m in range(M):\n",
    "            distances[n] += distance_table[PQ_code[n][m]][m] # Sum the partial distances from all the segments.\n",
    "            \n",
    "    return distance_table, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a37cf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UsefulSoftware\\Anaconda\\lib\\site-packages\\scipy\\cluster\\vq.py:603: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  warnings.warn(\"One of the clusters is empty. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.125426  , 2.2285783 , 1.7902957 , ..., 2.5514967 , 2.2632618 ,\n",
       "        1.752617  ],\n",
       "       [2.4390829 , 1.620167  , 1.7928649 , ..., 1.1864169 , 1.7381495 ,\n",
       "        1.2336261 ],\n",
       "       [1.3950148 , 1.8986391 , 1.2117358 , ..., 1.9527086 , 2.7055287 ,\n",
       "        2.8457036 ],\n",
       "       ...,\n",
       "       [2.4354773 , 3.154084  , 1.1651919 , ..., 2.6248562 , 2.2503722 ,\n",
       "        1.076196  ],\n",
       "       [0.8399424 , 1.4560368 , 1.701608  , ..., 2.8262062 , 2.3308375 ,\n",
       "        1.3183129 ],\n",
       "       [2.0058787 , 0.98286813, 1.3074644 , ..., 2.6367848 , 1.6629539 ,\n",
       "        1.7150615 ]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test case\n",
    "\n",
    "M = 8                     # Number of segments\n",
    "k = 256                   # Number of centroids per segment\n",
    "vector_dim = 128          # Dimension (length) of a vector\n",
    "total_vectors = 10000   # Number of database vectors\n",
    "\n",
    "# Generate random vectors\n",
    "np.random.seed(2022)\n",
    "vectors = np.random.random((total_vectors, vector_dim)).astype(np.float32)   # Database vectors\n",
    "q = np.random.random((vector_dim, )).astype(np.float32)                      # Query vector\n",
    "\n",
    "# Train, encode and search with Product Quantization\n",
    "codebook = PQ_train(vectors, M, k)\n",
    "PQ_code = PQ_encode(vectors, codebook)\n",
    "distance_table, distances = PQ_search(q, codebook, PQ_code)\n",
    "\n",
    "distance_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87afd452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 61,  21,  61,  88],\n",
       "       [ 61,  88, 126,  61],\n",
       "       [ 21, 126,  21,  61],\n",
       "       ...,\n",
       "       [131,  69,  58, 172],\n",
       "       [ 69,  69,  69, 172],\n",
       "       [ 58, 212, 159, 170]], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "  \n",
    "df = pd.read_csv('Polarity.csv')\n",
    "df.head()\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['Data'])\n",
    "n_subvectors = 4\n",
    "subvector_size = X.shape[1] // n_subvectors\n",
    "subvectors = np.array([X[:, i*subvector_size:(i+1)*subvector_size].toarray() for i in range(n_subvectors)])\n",
    "\n",
    "# Train a k-means clustering model on each subvector to obtain a codebook of subvector centroids\n",
    "n_clusters = 256\n",
    "codebooks = []\n",
    "for subvector in subvectors:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=1, random_state=0).fit(subvector)\n",
    "    codebooks.append(kmeans.cluster_centers_)\n",
    "\n",
    "# Encode each subvector using the corresponding codebook to obtain a vector of subvector indices\n",
    "subvector_indices = []\n",
    "for i, subvector in enumerate(subvectors):\n",
    "    nn = NearestNeighbors(n_neighbors=1, algorithm='brute', metric='euclidean').fit(codebooks[i])\n",
    "    _, indices = nn.kneighbors(subvector)\n",
    "    subvector_indices.append(indices.flatten())\n",
    "pq_codes = np.concatenate(subvector_indices, axis=0).reshape(-1, n_subvectors)\n",
    "# pq_codes.shape\n",
    "pq_codes_norm = normalize(pq_codes)\n",
    "# pq_codes\n",
    "\n",
    "codebook = PQ_train(vectors, M, k)\n",
    "PQ_code = PQ_encode(vectors, codebook)\n",
    "distance_table, distances = PQ_search(q, codebook, PQ_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98f99806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Data</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>__TiffanyAndCo Cousinnnn the return coming soon</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>at the balenciaga thinking about my friends fa...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>TiffanyAndCo bracelet I bought in Milan in Oct...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>QueenMoniB personifier channel_gibbs eccentric...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Pup_Dior_ Happy Valentines Day  You are so gor...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4295</th>\n",
       "      <td>2</td>\n",
       "      <td>Tag  timestamp \\n\\nLouis Vuitton pastel camo s...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4296</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the best place to buy a Rolex at in F...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4297</th>\n",
       "      <td>2</td>\n",
       "      <td>Suggest a Book Can you recommend me books abo...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4298</th>\n",
       "      <td>2</td>\n",
       "      <td>Chloe wears Louis Vuitton in HUNGER Magazine</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>0</td>\n",
       "      <td>Has a video or image of someone wearing a bag ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Polarity                                               Data  \\\n",
       "0            4   __TiffanyAndCo Cousinnnn the return coming soon    \n",
       "1            4  at the balenciaga thinking about my friends fa...   \n",
       "2            0  TiffanyAndCo bracelet I bought in Milan in Oct...   \n",
       "3            2  QueenMoniB personifier channel_gibbs eccentric...   \n",
       "4            4  Pup_Dior_ Happy Valentines Day  You are so gor...   \n",
       "...        ...                                                ...   \n",
       "4295         2  Tag  timestamp \\n\\nLouis Vuitton pastel camo s...   \n",
       "4296         2   What is the best place to buy a Rolex at in F...   \n",
       "4297         2   Suggest a Book Can you recommend me books abo...   \n",
       "4298         2       Chloe wears Louis Vuitton in HUNGER Magazine   \n",
       "4299         0  Has a video or image of someone wearing a bag ...   \n",
       "\n",
       "                                                 Vector  \n",
       "0     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "4295  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4296  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4297  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4298  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4299  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[4300 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "  \n",
    "df = pd.read_csv('Polarity.csv')\n",
    "df.head()\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['Data'])\n",
    "# Preparing the data to fit_tranformation\n",
    "listofData  = list(df[\"Data\"].array)\n",
    "\n",
    "# Create the Matrix\n",
    "matrix = vectorizer.fit_transform(listofData)\n",
    "matrix = matrix.toarray()\n",
    "\n",
    "# Add into new column in df\n",
    "df[\"Vector\"] = [row.tolist() for row in matrix]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07307138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
