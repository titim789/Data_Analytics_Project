{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\26939\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\26939\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'Dataset/job_descriptions.csv'\n",
    "pd.read_csv(dataset_path).head(100).to_csv('sample_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:51: SyntaxWarning: invalid escape sequence '\\j'\n",
      "<>:51: SyntaxWarning: invalid escape sequence '\\j'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15784\\1308772417.py:51: SyntaxWarning: invalid escape sequence '\\j'\n",
      "  binary_df_chunk.to_csv(f'Dataset\\job_posting_chunks\\jpc{i}.csv', index=False)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0\n",
      "Chunk 0 processed and saved.\n",
      "Processing chunk 1\n",
      "Chunk 1 processed and saved.\n",
      "Processing chunk 2\n",
      "Chunk 2 processed and saved.\n",
      "Processing chunk 3\n",
      "Chunk 3 processed and saved.\n",
      "Processing chunk 4\n",
      "Chunk 4 processed and saved.\n",
      "Processing chunk 5\n",
      "Chunk 5 processed and saved.\n",
      "Processing chunk 6\n",
      "Chunk 6 processed and saved.\n",
      "Processing chunk 7\n",
      "Chunk 7 processed and saved.\n",
      "Processing chunk 8\n",
      "Chunk 8 processed and saved.\n",
      "Processing chunk 9\n",
      "Chunk 9 processed and saved.\n",
      "Processing chunk 10\n",
      "Chunk 10 processed and saved.\n",
      "Processing chunk 11\n",
      "Chunk 11 processed and saved.\n",
      "Processing chunk 12\n",
      "Chunk 12 processed and saved.\n",
      "Processing chunk 13\n",
      "Chunk 13 processed and saved.\n",
      "Processing chunk 14\n",
      "Chunk 14 processed and saved.\n",
      "Processing chunk 15\n",
      "Chunk 15 processed and saved.\n",
      "Processing chunk 16\n",
      "Chunk 16 processed and saved.\n",
      "Processing chunk 17\n",
      "Chunk 17 processed and saved.\n",
      "Processing chunk 18\n",
      "Chunk 18 processed and saved.\n",
      "Processing chunk 19\n",
      "Chunk 19 processed and saved.\n",
      "Processing chunk 20\n",
      "Chunk 20 processed and saved.\n",
      "Processing chunk 21\n",
      "Chunk 21 processed and saved.\n",
      "Processing chunk 22\n",
      "Chunk 22 processed and saved.\n",
      "Processing chunk 23\n",
      "Chunk 23 processed and saved.\n",
      "Processing chunk 24\n",
      "Chunk 24 processed and saved.\n",
      "Processing chunk 25\n",
      "Chunk 25 processed and saved.\n",
      "Processing chunk 26\n",
      "Chunk 26 processed and saved.\n",
      "Processing chunk 27\n",
      "Chunk 27 processed and saved.\n",
      "Processing chunk 28\n",
      "Chunk 28 processed and saved.\n",
      "Processing chunk 29\n",
      "Chunk 29 processed and saved.\n",
      "Processing chunk 30\n",
      "Chunk 30 processed and saved.\n",
      "Processing chunk 31\n",
      "Chunk 31 processed and saved.\n",
      "Processing chunk 32\n",
      "Chunk 32 processed and saved.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Get the English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a tokenizer function that also removes stopwords\n",
    "def tokenize(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and non-alphabetic tokens\n",
    "    # tokens = [stemmer.stem(word) for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    tokens = [word for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Initialize the CountVectorizer with the custom tokenizer\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize, binary=True, lowercase=True)\n",
    "\n",
    "# Set the chunk size\n",
    "chunk_size = 50000  # Adjust this value based on your memory capacity\n",
    "\n",
    "dataset_path = 'Dataset/job_descriptions.csv'\n",
    "fit_sample = pd.read_csv(dataset_path, nrows=1000)  # Adjust nrows as needed\n",
    "\n",
    "# tokenized_skills = fit_sample['skills'].apply(lambda x: ' '.join(tokenize(x)))\n",
    "\n",
    "all_text_fit =  fit_sample['Job Title']\n",
    "vectorizer.fit(all_text_fit)\n",
    "chunks = pd.read_csv(dataset_path, chunksize=chunk_size, usecols=['Job Title'])\n",
    "\n",
    "# Process each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'Processing chunk {i}')\n",
    "    # Concatenate all text columns into one to vectorize\n",
    "    # Tokenize 'Job Title' and 'Role' columns\n",
    "    tokenized_job_titles = chunk['Job Title'].apply(lambda x: ' '.join(tokenize(x)))\n",
    "    # tokenized_roles = chunk['Role'].apply(lambda x: ' '.join(tokenize(x)))\n",
    "\n",
    "    # Concatenate tokenized 'Job Title' and 'Role'\n",
    "    all_text = tokenized_job_titles\n",
    "\n",
    "    # all_text = chunk['Job Title'] + \" \" + chunk['Role']\n",
    "    \n",
    "    # Fit the vectorizer and transform the text into a binary matrix\n",
    "    X_chunk = vectorizer.transform(all_text)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    binary_df_chunk = pd.DataFrame(X_chunk.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # Save the binary dataframe chunk to a CSV file\n",
    "    binary_df_chunk.to_csv(f'Dataset\\job_posting_chunks\\jpc{i}.csv', index=False)\n",
    "\n",
    "    print(f'Chunk {i} processed and saved.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Digital Marketing Specialist Social Media Manager\n",
       "1                   Web Developer Frontend Web Developer\n",
       "2             Operations Manager Quality Control Manager\n",
       "3             Network Engineer Wireless Network Engineer\n",
       "4                       Event Manager Conference Manager\n",
       "                             ...                        \n",
       "995                  Urban Planner Environmental Planner\n",
       "996       Mechanical Engineer Mechanical Design Engineer\n",
       "997                               IT Manager IT Director\n",
       "998                     UI Developer Front-End Developer\n",
       "999    Landscape Architect Residential Landscape Desi...\n",
       "Length: 1000, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('Dataset/job_posting_chunks/jpc0.csv').head(100).to_csv('sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APRIORI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chunk(chunk):\n",
    "    transactions = []\n",
    "    # Iterate over each row in the chunk\n",
    "    for index, row in chunk.iterrows():\n",
    "        # Create a set for the current transaction\n",
    "        transaction = frozenset(row.index[row == 1].tolist())\n",
    "        transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "# sample_data = pd.read_csv(sample_file_path)\n",
    "# test_transactions = preprocess_chunk(sample_data.drop(columns=sample_data.columns[0]))\n",
    "# test_transactions[:5]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count support for each item in a chunk\n",
    "def count_support(transactions):\n",
    "    item_count = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_count[item] += 1\n",
    "    # print(f\"Count Support: {item_count}\")\n",
    "    return item_count\n",
    "\n",
    "# Update the process_chunks function to handle the binary matrix format\n",
    "def process_chunks(chunk_paths):\n",
    "    global_item_counts = defaultdict(int)\n",
    "    for path in chunk_paths:\n",
    "        chunk = pd.read_csv(path, index_col=0)  # Read the chunk and set the first column as index\n",
    "        transactions = preprocess_chunk(chunk)\n",
    "        local_item_counts = count_support(transactions)\n",
    "        # Aggregate the counts from this chunk to the global counts\n",
    "        for item, count in local_item_counts.items():\n",
    "            global_item_counts[item] += count\n",
    "    return global_item_counts\n",
    "\n",
    "# Since we only have one sample file for now, we'll create a list containing just that path\n",
    "# chunk_paths = [sample_file_path]\n",
    "# Call the process_chunks function with the single file path\n",
    "# global_item_counts = process_chunks(chunk_paths)\n",
    "\n",
    "# Show the global item counts to verify if the function works correctly\n",
    "# global_item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total transactions from chunks\n",
    "def get_total_transactions(chunk_paths):\n",
    "    total_transactions = 0\n",
    "    for path in chunk_paths:\n",
    "        with open(path, 'r') as file:\n",
    "            for line in file:\n",
    "                total_transactions += 1\n",
    "    print(f\"Total Transactions: {total_transactions-1}\")\n",
    "    return total_transactions - 1\n",
    "\n",
    "# Identify frequent itemsets from global counts\n",
    "def get_frequent_itemsets(global_counts, min_support, total_transactions):\n",
    "    frequent_itemsets = set()\n",
    "    for candidate, count in global_counts.items():\n",
    "        if count / total_transactions >= min_support:\n",
    "           frequent_itemsets.add(frozenset[candidate])\n",
    "    return frequent_itemsets\n",
    "\n",
    "# Step 2: Generate candidate k-itemsets\n",
    "def generate_candidates(frequent_itemsets, k):\n",
    "    candidates = set()\n",
    "    frequent_itemsets_list = list(frequent_itemsets)  # Convert to list to allow indexing.\n",
    "\n",
    "    for i in range(len(frequent_itemsets_list)):\n",
    "        for j in range(i+1, len(frequent_itemsets_list)):\n",
    "            itemset1 = frequent_itemsets_list[i]\n",
    "            itemset2 = frequent_itemsets_list[j]\n",
    "            # Ensure that both itemset1 and itemset2 are frozenset instances\n",
    "            if isinstance(itemset1, frozenset) and isinstance(itemset2, frozenset):\n",
    "                union_itemset = itemset1.union(itemset2)\n",
    "                intersection_itemset = itemset1.intersection(itemset2)\n",
    "                \n",
    "                if len(union_itemset) == k and len(intersection_itemset) == k-2:\n",
    "                    candidates.add(union_itemset)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "# Step 3: Prune candidate k-itemsets that dont have all k-1 frequent subsets\n",
    "def generate_pruned_candidates(candidates,frequent_itemsets, k):\n",
    "    pruned_candidates = set()\n",
    "    for candidate in candidates:\n",
    "        is_frequent = True\n",
    "        for subset in combinations(candidate, k - 1):\n",
    "            subset_frozen = frozenset(subset)\n",
    "\n",
    "            # as long as one is not frequent, break and continue\n",
    "            if subset_frozen not in frequent_itemsets:\n",
    "                is_frequent = False\n",
    "                break\n",
    "\n",
    "        # if all subsets are frequent, add to pruned\n",
    "        if is_frequent:\n",
    "            pruned_candidates.add(candidate)\n",
    "    return pruned_candidates\n",
    "\n",
    "def get_candidate_counts(chunks_paths,candidates):\n",
    "    candidate_counts = defaultdict(int)\n",
    "    for path in chunks_paths:\n",
    "        chunk = pd.read_csv(path, index_col=0)\n",
    "        transactions = preprocess_chunk(chunk)\n",
    "\n",
    "        for transaction in transactions:\n",
    "            for candidate in candidates:\n",
    "                if candidate.issubset(transaction):\n",
    "                    candidate_counts[candidate] += 1\n",
    "    return candidate_counts\n",
    "\n",
    "def get_frequent_itemsets_from_candidates(candidate_counts, min_support, total_transactions):\n",
    "    frequent_itemsets = set()\n",
    "    for candidate, count in candidate_counts.items():\n",
    "        if (count / total_transactions) >= min_support:\n",
    "            frequent_itemsets.add(candidate)\n",
    "    return frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {frozenset['aerospace'], frozenset['accountant'], frozenset['interior'], frozenset['process'], frozenset['administrative'], frozenset['architectural'], frozenset['agent'], frozenset['consultant'], frozenset['family'], frozenset['director'], frozenset['associate'], frozenset['chemical'], frozenset['business'], frozenset['assistant'], frozenset['mechanical'], frozenset['pediatrician'], frozenset['qa'], frozenset['hr'], frozenset['speech'], frozenset['assurance'], frozenset['chain'], frozenset['researcher'], frozenset['registered'], frozenset['manager'], frozenset['structural'], frozenset['advisor'], frozenset['counselor'], frozenset['technician'], frozenset['customer'], frozenset['finance'], frozenset['media'], frozenset['inventory'], frozenset['pharmaceutical'], frozenset['civil'], frozenset['ux'], frozenset['coordinator'], frozenset['generalist'], frozenset['representative'], frozenset['product'], frozenset['counsel'], frozenset['relations'], frozenset['social'], frozenset['designer'], frozenset['physician'], frozenset['public'], frozenset['service'], frozenset['administrator'], frozenset['software'], frozenset['development'], frozenset['writer'], frozenset['therapist'], frozenset['practitioner'], frozenset['technical'], frozenset['key'], frozenset['support'], frozenset['specialist'], frozenset['occupational'], frozenset['litigation'], frozenset['environmental'], frozenset['nurse'], frozenset['legal'], frozenset['systems'], frozenset['scientist'], frozenset['banker'], frozenset['wedding'], frozenset['lawyer'], frozenset['physical'], frozenset['entry'], frozenset['art'], frozenset['purchasing'], frozenset['security'], frozenset['human'], frozenset['financial'], frozenset['project'], frozenset['tester'], frozenset['market'], frozenset['tax'], frozenset['sem'], frozenset['paralegal'], frozenset['electrical'], frozenset['teacher'], frozenset['resources'], frozenset['planner'], frozenset['success'], frozenset['network'], frozenset['psychologist'], frozenset['attorney'], frozenset['java'], frozenset['ui'], frozenset['seo'], frozenset['content'], frozenset['controller'], frozenset['brand'], frozenset['web'], frozenset['analyst'], frozenset['data'], frozenset['dental'], frozenset['architect'], frozenset['personal'], frozenset['investment'], frozenset['operations'], frozenset['event'], frozenset['worker'], frozenset['executive'], frozenset['substance'], frozenset['ambassador'], frozenset['database'], frozenset['copywriter'], frozenset['graphic'], frozenset['engineer'], frozenset['landscape'], frozenset['sales'], frozenset['office'], frozenset['developer'], frozenset['hygienist'], frozenset['clerk'], frozenset['digital'], frozenset['urban'], frozenset['research'], frozenset['account'], frozenset['procurement'], frozenset['veterinarian'], frozenset['email'], frozenset['quality'], frozenset['marketing'], frozenset['supply']}}\n"
     ]
    }
   ],
   "source": [
    "def apriori_algorithm(files, min_support):\n",
    "    # Step 1: Process all chunks and get global counts and total transactions\n",
    "    global_item_counts = defaultdict(int)\n",
    "    total_transactions = 0\n",
    "    for file_path in files:\n",
    "        chunk = pd.read_csv(file_path, index_col=0)\n",
    "        transactions = preprocess_chunk(chunk)\n",
    "        local_item_counts = count_support(transactions)\n",
    "        for item, count in local_item_counts.items():\n",
    "            global_item_counts[item] += count\n",
    "        total_transactions += len(chunk)\n",
    "\n",
    "    # Step 2: Find frequent 1-itemsets\n",
    "    frequent_itemsets = get_frequent_itemsets(global_item_counts, min_support, total_transactions)\n",
    "    all_frequent_itemsets = {1: frequent_itemsets}\n",
    "\n",
    "    # Step 3: Iteratively find all frequent itemsets\n",
    "    k = 2\n",
    "    while True:\n",
    "        # Generate candidate k-itemsets\n",
    "        candidates = generate_candidates(all_frequent_itemsets[k-1], k)\n",
    "        # Prune candidate k-itemsets\n",
    "        pruned_candidates = generate_pruned_candidates(candidates, all_frequent_itemsets[k-1], k)\n",
    "        # Count the support of each pruned candidate itemset across all files\n",
    "        candidate_counts = defaultdict(int)\n",
    "        for file_path in files:\n",
    "            chunk = pd.read_csv(file_path, index_col=0)\n",
    "            transactions = preprocess_chunk(chunk)\n",
    "            for candidate in pruned_candidates:\n",
    "                if candidate.issubset(transactions):\n",
    "                    candidate_counts[candidate] += 1\n",
    "        # Identify the frequent k-itemsets\n",
    "        frequent_k_itemsets = get_frequent_itemsets_from_candidates(candidate_counts, min_support, total_transactions)\n",
    "        if frequent_k_itemsets:\n",
    "            all_frequent_itemsets[k] = frequent_k_itemsets\n",
    "        else:\n",
    "            break\n",
    "        k += 1\n",
    "\n",
    "    return all_frequent_itemsets\n",
    "\n",
    "# List of file paths\n",
    "files = [f\"Dataset/job_posting_chunks/jpc{i}.csv\" for i in range(0, 33)]  # Adjust the pattern as needed\n",
    "\n",
    "# Run the Apriori algorithm\n",
    "min_support = 0.00001  # Adjust the minimum support threshold as needed\n",
    "frequent_itemsets = apriori_algorithm(files, min_support)\n",
    "\n",
    "# Display the frequent itemsets\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Frequent 1-itemsets ==\n",
      "*frozenset['aerospace']\n",
      "*frozenset['accountant']\n",
      "*frozenset['interior']\n",
      "*frozenset['process']\n",
      "*frozenset['administrative']\n",
      "*frozenset['architectural']\n",
      "*frozenset['agent']\n",
      "*frozenset['consultant']\n",
      "*frozenset['family']\n",
      "*frozenset['director']\n",
      "*frozenset['associate']\n",
      "*frozenset['chemical']\n",
      "*frozenset['business']\n",
      "*frozenset['assistant']\n",
      "*frozenset['mechanical']\n",
      "*frozenset['pediatrician']\n",
      "*frozenset['qa']\n",
      "*frozenset['hr']\n",
      "*frozenset['speech']\n",
      "*frozenset['assurance']\n",
      "*frozenset['chain']\n",
      "*frozenset['researcher']\n",
      "*frozenset['registered']\n",
      "*frozenset['manager']\n",
      "*frozenset['structural']\n",
      "*frozenset['advisor']\n",
      "*frozenset['counselor']\n",
      "*frozenset['technician']\n",
      "*frozenset['customer']\n",
      "*frozenset['finance']\n",
      "*frozenset['media']\n",
      "*frozenset['inventory']\n",
      "*frozenset['pharmaceutical']\n",
      "*frozenset['civil']\n",
      "*frozenset['ux']\n",
      "*frozenset['coordinator']\n",
      "*frozenset['generalist']\n",
      "*frozenset['representative']\n",
      "*frozenset['product']\n",
      "*frozenset['counsel']\n",
      "*frozenset['relations']\n",
      "*frozenset['social']\n",
      "*frozenset['designer']\n",
      "*frozenset['physician']\n",
      "*frozenset['public']\n",
      "*frozenset['service']\n",
      "*frozenset['administrator']\n",
      "*frozenset['software']\n",
      "*frozenset['development']\n",
      "*frozenset['writer']\n",
      "*frozenset['therapist']\n",
      "*frozenset['practitioner']\n",
      "*frozenset['technical']\n",
      "*frozenset['key']\n",
      "*frozenset['support']\n",
      "*frozenset['specialist']\n",
      "*frozenset['occupational']\n",
      "*frozenset['litigation']\n",
      "*frozenset['environmental']\n",
      "*frozenset['nurse']\n",
      "*frozenset['legal']\n",
      "*frozenset['systems']\n",
      "*frozenset['scientist']\n",
      "*frozenset['banker']\n",
      "*frozenset['wedding']\n",
      "*frozenset['lawyer']\n",
      "*frozenset['physical']\n",
      "*frozenset['entry']\n",
      "*frozenset['art']\n",
      "*frozenset['purchasing']\n",
      "*frozenset['security']\n",
      "*frozenset['human']\n",
      "*frozenset['financial']\n",
      "*frozenset['project']\n",
      "*frozenset['tester']\n",
      "*frozenset['market']\n",
      "*frozenset['tax']\n",
      "*frozenset['sem']\n",
      "*frozenset['paralegal']\n",
      "*frozenset['electrical']\n",
      "*frozenset['teacher']\n",
      "*frozenset['resources']\n",
      "*frozenset['planner']\n",
      "*frozenset['success']\n",
      "*frozenset['network']\n",
      "*frozenset['psychologist']\n",
      "*frozenset['attorney']\n",
      "*frozenset['java']\n",
      "*frozenset['ui']\n",
      "*frozenset['seo']\n",
      "*frozenset['content']\n",
      "*frozenset['controller']\n",
      "*frozenset['brand']\n",
      "*frozenset['web']\n",
      "*frozenset['analyst']\n",
      "*frozenset['data']\n",
      "*frozenset['dental']\n",
      "*frozenset['architect']\n",
      "*frozenset['personal']\n",
      "*frozenset['investment']\n",
      "*frozenset['operations']\n",
      "*frozenset['event']\n",
      "*frozenset['worker']\n",
      "*frozenset['executive']\n",
      "*frozenset['substance']\n",
      "*frozenset['ambassador']\n",
      "*frozenset['database']\n",
      "*frozenset['copywriter']\n",
      "*frozenset['graphic']\n",
      "*frozenset['engineer']\n",
      "*frozenset['landscape']\n",
      "*frozenset['sales']\n",
      "*frozenset['office']\n",
      "*frozenset['developer']\n",
      "*frozenset['hygienist']\n",
      "*frozenset['clerk']\n",
      "*frozenset['digital']\n",
      "*frozenset['urban']\n",
      "*frozenset['research']\n",
      "*frozenset['account']\n",
      "*frozenset['procurement']\n",
      "*frozenset['veterinarian']\n",
      "*frozenset['email']\n",
      "*frozenset['quality']\n",
      "*frozenset['marketing']\n",
      "*frozenset['supply']\n",
      "\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "def print_frequent_itemsets(frequent_itemsets):\n",
    "    count = 0\n",
    "    for k, itemsets in frequent_itemsets.items():\n",
    "        print(f\"== Frequent {k}-itemsets ==\")\n",
    "        for itemset in itemsets:\n",
    "            # Ensure all elements are strings, convert frozenset to a sorted list and then to a string for printing\n",
    "            itemset_list = [str(item) for item in sorted(list(itemset))]\n",
    "            itemset_str = \", \".join(itemset_list)\n",
    "            print(f\"{itemset_str}\")\n",
    "            count+=1\n",
    "        print(\"\")  # Print a newline for better readability between different levels\n",
    "    return count\n",
    "        \n",
    "# Example usage:\n",
    "# Assuming 'frequent_itemsets' is your dictionary of frequent itemsets\n",
    "count = print_frequent_itemsets(frequent_itemsets)\n",
    "print(count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
