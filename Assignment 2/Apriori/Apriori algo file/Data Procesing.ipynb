{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Processing for Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def clean_and_split_job_titles(title_str):\n",
    "#     # Basic cleaning: strip leading/trailing spaces, convert to lowercase, and split into words\n",
    "#     return title_str.strip().lower().split()\n",
    "\n",
    "# def process_job_title_chunks(input_path, chunk_size=100000):\n",
    "#     chunks = pd.read_csv(input_path, chunksize=chunk_size, usecols=['Job Title'])\n",
    "\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#         print(f'Processing chunk {i}')\n",
    "#         # Clean and split the 'Job Title' column\n",
    "#         chunk['Job Title'] = chunk['Job Title'].dropna().apply(clean_and_split_job_titles)\n",
    "\n",
    "#         # Save to a CSV file\n",
    "#         output_file = f'Dataset/job_title_split_chunks/job_title_split_chunk_{i}.csv'\n",
    "#         chunk['Job Title'].to_csv(output_file, index=False, header=False)\n",
    "\n",
    "#         print(f'Chunk {i} saved to {output_file}')\n",
    "\n",
    "# # Path to your dataset\n",
    "# dataset_path = 'Dataset/job_descriptions.csv'\n",
    "\n",
    "# # Process the dataset\n",
    "# process_job_title_chunks(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def clean_and_split_job_titles(title_str):\n",
    "#     # Basic cleaning: strip leading/trailing spaces, convert to lowercase, and split into words\n",
    "#     return title_str.strip().lower().split()\n",
    "\n",
    "# def create_dataset(input_path, output_path):\n",
    "#     data = pd.read_csv(input_path, usecols=['Job Title'])\n",
    "\n",
    "#     # Apply the cleaning and splitting function\n",
    "#     split_titles = data['Job Title'].dropna().apply(clean_and_split_job_titles)\n",
    "\n",
    "#     # Determine the maximum number of words in any title\n",
    "#     max_words = split_titles.apply(len).max()\n",
    "\n",
    "#     # Create a DataFrame with each word in a separate column\n",
    "#     titles_df = pd.DataFrame(split_titles.tolist(), index=split_titles.index)\n",
    "#     titles_df = titles_df.reindex(columns=range(max_words))  # Add missing columns for shorter titles\n",
    "\n",
    "#     # Save to CSV\n",
    "#     titles_df.to_csv(output_path, index=False, header=False)\n",
    "\n",
    "# input_path = '../Dataset/job_descriptions.csv'\n",
    "# output_path = '../Dataset/job_title_split.csv'\n",
    "# create_dataset(input_path,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def chunk_stuff(input_file,output_path, chunk_size=500):\n",
    "    chunks = pd.read_csv(input_file, chunksize=chunk_size, usecols=['Job Title'])\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        output_file = f'{output_path}/{input_file}_chunk_{i}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Processing for Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "zip_file_path = '../Dataset/transactions.zip'\n",
    "csv_file_name = 'transactions.csv'\n",
    "output_path = '../Dataset/transactions_split_chunks/'\n",
    "selected_columns = ['CUST_ID', 'EXP_TYPE']\n",
    "chunk_size = 2500\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "    with z.open(csv_file_name) as csv_file:\n",
    "        df = pd.read_csv(csv_file, usecols=selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUST_ID</th>\n",
       "      <th>EXP_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C000BK8N2S</td>\n",
       "      <td>Entertainment|Entertainment|Entertainment|Gamb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C000TDGP4R</td>\n",
       "      <td>Entertainment|Entertainment|Entertainment|Ente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C001F6USSU</td>\n",
       "      <td>Entertainment|Health|Entertainment|Clothing|En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0026REM1Q</td>\n",
       "      <td>Groceries|Entertainment|Entertainment|Entertai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C002LR5U74</td>\n",
       "      <td>Groceries|Entertainment|Health|Groceries|Enter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74995</th>\n",
       "      <td>CZZUM94ZSD</td>\n",
       "      <td>Entertainment|Entertainment|Motor/Travel|Motor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74996</th>\n",
       "      <td>CZZW4TZZ3H</td>\n",
       "      <td>Entertainment|Bills and Utilities|Entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74997</th>\n",
       "      <td>CZZXQ5ULG7</td>\n",
       "      <td>Groceries|Groceries|Entertainment|Groceries|Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74998</th>\n",
       "      <td>CZZYEH2PF6</td>\n",
       "      <td>Groceries|Entertainment|Groceries|Groceries|Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74999</th>\n",
       "      <td>CZZYMEA7MJ</td>\n",
       "      <td>Housing|Groceries|Bills and Utilities|Grocerie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CUST_ID                                           EXP_TYPE\n",
       "0      C000BK8N2S  Entertainment|Entertainment|Entertainment|Gamb...\n",
       "1      C000TDGP4R  Entertainment|Entertainment|Entertainment|Ente...\n",
       "2      C001F6USSU  Entertainment|Health|Entertainment|Clothing|En...\n",
       "3      C0026REM1Q  Groceries|Entertainment|Entertainment|Entertai...\n",
       "4      C002LR5U74  Groceries|Entertainment|Health|Groceries|Enter...\n",
       "...           ...                                                ...\n",
       "74995  CZZUM94ZSD  Entertainment|Entertainment|Motor/Travel|Motor...\n",
       "74996  CZZW4TZZ3H  Entertainment|Bills and Utilities|Entertainmen...\n",
       "74997  CZZXQ5ULG7  Groceries|Groceries|Entertainment|Groceries|Gr...\n",
       "74998  CZZYEH2PF6  Groceries|Entertainment|Groceries|Groceries|Gr...\n",
       "74999  CZZYMEA7MJ  Housing|Groceries|Bills and Utilities|Grocerie...\n",
       "\n",
       "[75000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df = df.groupby('CUST_ID')['EXP_TYPE'].apply(lambda x: '|'.join(x)).reset_index()\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [Entertainment, Entertainment, Entertainment, ...\n",
       "1        [Entertainment, Entertainment, Entertainment, ...\n",
       "2        [Entertainment, Health, Entertainment, Clothin...\n",
       "3        [Groceries, Entertainment, Entertainment, Ente...\n",
       "4        [Groceries, Entertainment, Health, Groceries, ...\n",
       "                               ...                        \n",
       "74995    [Entertainment, Entertainment, Motor/Travel, M...\n",
       "74996    [Entertainment, Bills and Utilities, Entertain...\n",
       "74997    [Groceries, Groceries, Entertainment, Grocerie...\n",
       "74998    [Groceries, Entertainment, Groceries, Grocerie...\n",
       "74999    [Housing, Groceries, Bills and Utilities, Groc...\n",
       "Name: EXP_TYPE, Length: 75000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_grouped_df = grouped_df['EXP_TYPE'].apply(lambda x: x.split('|')).apply(list)\n",
    "new_grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_0.csv\n",
      "Chunk 1 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_1.csv\n",
      "Chunk 2 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_2.csv\n",
      "Chunk 3 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_3.csv\n",
      "Chunk 4 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_4.csv\n",
      "Chunk 5 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_5.csv\n",
      "Chunk 6 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_6.csv\n",
      "Chunk 7 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_7.csv\n",
      "Chunk 8 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_8.csv\n",
      "Chunk 9 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_9.csv\n",
      "Chunk 10 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_10.csv\n",
      "Chunk 11 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_11.csv\n",
      "Chunk 12 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_12.csv\n",
      "Chunk 13 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_13.csv\n",
      "Chunk 14 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_14.csv\n",
      "Chunk 15 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_15.csv\n",
      "Chunk 16 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_16.csv\n",
      "Chunk 17 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_17.csv\n",
      "Chunk 18 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_18.csv\n",
      "Chunk 19 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_19.csv\n",
      "Chunk 20 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_20.csv\n",
      "Chunk 21 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_21.csv\n",
      "Chunk 22 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_22.csv\n",
      "Chunk 23 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_23.csv\n",
      "Chunk 24 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_24.csv\n",
      "Chunk 25 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_25.csv\n",
      "Chunk 26 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_26.csv\n",
      "Chunk 27 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_27.csv\n",
      "Chunk 28 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_28.csv\n",
      "Chunk 29 saved to ../Dataset/transactions_split_chunks/transactions_split_chunk_29.csv\n"
     ]
    }
   ],
   "source": [
    "# Process and save each chunk\n",
    "for i in range(0, len(new_grouped_df), chunk_size):\n",
    "    end = min(i + chunk_size, len(new_grouped_df))\n",
    "\n",
    "    chunk = new_grouped_df[i:end]\n",
    "\n",
    "    output_file = f'{output_path}transactions_split_chunk_{i//chunk_size}.csv'\n",
    "    chunk.to_csv(output_file, index=False, header= False)\n",
    "    print(f'Chunk {i//chunk_size} saved to {output_file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
