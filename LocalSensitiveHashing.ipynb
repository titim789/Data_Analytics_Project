{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LfYW-mWOCeOi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HNZm-LufF2o4"
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"job_postings_prepped.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "vectorizer = pickle.load(open('vectorizer.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH **Calculations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragne\\AppData\\Local\\Temp\\ipykernel_22204\\3049473867.py:17: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  hash_functions[index] = random_vector\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hash_Functions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.4621593152243012, -0.35672951157632493, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.47087120585137815, 1.401192565415804, 0.946...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.9023205094063987, -0.9520906006374974, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.27129118011215153, -0.9232229045554883, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.14656172513477275, -0.39618645740627173, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.8966847254456627, -0.5544146627707663, -1.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.6977282567135773, -0.540626667009899, 0.286...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[-0.894841459583318, -1.5152027984681657, 0.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.798601012942948, 0.2437842899682288, -0.557...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[-0.7675097294221432, -0.5598300776180332, -1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Hash_Functions\n",
       "0  [-0.4621593152243012, -0.35672951157632493, 1....\n",
       "1  [0.47087120585137815, 1.401192565415804, 0.946...\n",
       "2  [-0.9023205094063987, -0.9520906006374974, -0....\n",
       "3  [0.27129118011215153, -0.9232229045554883, 0.5...\n",
       "4  [-0.14656172513477275, -0.39618645740627173, 1...\n",
       "5  [0.8966847254456627, -0.5544146627707663, -1.8...\n",
       "6  [0.6977282567135773, -0.540626667009899, 0.286...\n",
       "7  [-0.894841459583318, -1.5152027984681657, 0.61...\n",
       "8  [0.798601012942948, 0.2437842899682288, -0.557...\n",
       "9  [-0.7675097294221432, -0.5598300776180332, -1...."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "k = 10 #TODO: find out optimal value\n",
    "vector_dimensions = len(df['Vector'][0])\n",
    "non_zero_values = len(df['Vector'][0]) #TODO: find out optimal value\n",
    "\n",
    "def generate_hash_function(k,vector_dimensions,non_zero_values):\n",
    "    hash_functions_list = []\n",
    "\n",
    "    for i in range(k):\n",
    "        if i%10 == 0: print(\"Progress:\", str(i)+\"/\"+str(k))\n",
    "        hash_functions = np.zeros(vector_dimensions)\n",
    "        selected_indexes = random.sample(range(vector_dimensions),non_zero_values)\n",
    "        \n",
    "        for index in selected_indexes:\n",
    "            random_vector = np.random.randn(1)\n",
    "            hash_functions[index] = random_vector\n",
    "\n",
    "        hash_functions_list.append(hash_functions)\n",
    "\n",
    "    return hash_functions_list\n",
    "\n",
    "random_hash_functions = generate_hash_function(k,vector_dimensions,non_zero_values)\n",
    "random_hash_functions_df = pd.DataFrame({'Hash_Functions': random_hash_functions})\n",
    "random_hash_functions_df\n",
    "\n",
    "# random_hash_functions_df.to_json('LSH Search Results/LSH 10 Random Hash Functions.json', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "YofeI8XDNe8t",
    "outputId": "ec9b6cf4-1175-4d13-982b-b8cf03e96c50"
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# Perform dot product and get binary vectors\n",
    "def generate_binary_vectors(input_vector,random_hash_functions_df):\n",
    "    binary_vector = []\n",
    "    for i,row in random_hash_functions_df.iterrows():\n",
    "        input_vector = np.asarray(input_vector, dtype=np.float64)\n",
    "        hash_function = np.asarray(row['Hash_Functions'], dtype=np.float64)\n",
    "    \n",
    "        dot_product = np.dot(input_vector,hash_function)\n",
    "        # result = 0 if dot_product <= 0 else 1\n",
    "        result = np.where(dot_product <= 0, 0, 1)\n",
    "        binary_vector.append(result.tolist())\n",
    "    return binary_vector\n",
    "\n",
    "# Make a new column called Binary Vectors for all the binary vectors\n",
    "# random_hash_functions_df = pd.read_json('LSH Search Results/LSH Random Hash Functions.json')\n",
    "# df['Binary Vectors'] = df['Vector'].apply(lambda x : generate_binary_vectors(x,random_hash_functions_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_14292\\3669591667.py:17: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  hash_functions[index] = random_vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to generate binary vectors: 0.003000020980834961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "test_hash = pd.DataFrame({'Hash_Functions': generate_hash_function(2, 105248, 105248)})\n",
    "generate_binary_vectors(json.loads(df.iloc[0][\"Vector\"]), test_hash)\n",
    "# len(query_matrix[0])\n",
    "# json.loads(df.iloc[0][\"Vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear search on binary vector using hamming distance Dh\n",
    "def hamming_distance(query_vector,database_sample):\n",
    "  mismatched_bits = 0\n",
    "  for i in range(len(query_vector)):\n",
    "    # print(i)\n",
    "    # print('query vector = ',query_vector[i])\n",
    "    # print('database sample = ',database_sample[i])\n",
    "    if query_vector[i] != database_sample[i]:\n",
    "      mismatched_bits += 1\n",
    "  return mismatched_bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find Best K Value Time from 10 to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate K Value Time Comparison\n",
    "# k_time_dict = {}\n",
    "# vector_dimensions = len(df['Vector'][0])\n",
    "# non_zero_values = len(df['Vector'][0]) #TODO: find out optimal value\n",
    "\n",
    "# for k in range(10,101,10):\n",
    "#     start = time.time()\n",
    "\n",
    "#     random_hash_functions = generate_hash_function(k,vector_dimensions,non_zero_values)\n",
    "#     binary_vectors_list = df['Vector'].apply( lambda x : one_hashing_function(x,random_hash_functions))\n",
    "\n",
    "#     end = time.time()\n",
    "#     results_time = end - start\n",
    "#     k_time_dict[k] = results_time\n",
    "#     k_df = pd.DataFrame(list(k_time_dict.items()), columns=['K Value', 'Time'])\n",
    "#     k_df.to_csv(f'LSH Search Results/K_Value_Time.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate binary of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported  hash functions from Json file\n",
      "Index: [159, 215, 144, 31, 0, 33, 210, 299, 227, 133]\n",
      "Time Taken to generate binary vectors(k = 10): 340.71075892448425 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [56, 59, 46, 43, 84, 227, 172, 5, 165, 52, 92, 213, 176, 17, 67, 77, 96, 131, 175, 110, 254, 41, 66, 290, 287, 58, 6, 219, 4, 32]\n",
      "Time Taken to generate binary vectors(k = 30): 935.3069818019867 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [243, 60, 115, 254, 59, 296, 19, 299, 186, 155, 272, 257, 141, 54, 149, 266, 209, 193, 225, 221, 287, 146, 293, 196, 75, 67, 276, 175, 295, 142, 104, 22, 49, 114, 42, 203, 58, 6, 292, 55, 108, 82, 286, 217, 69, 268, 91, 48, 30, 93]\n",
      "Time Taken to generate binary vectors(k = 50): 1529.8507180213928 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [8, 53, 289, 133, 226, 21, 216, 295, 268, 296, 214, 165, 218, 61, 121, 243, 62, 140, 16, 143, 183, 223, 262, 30, 55, 110, 284, 91, 89, 77, 95, 168, 98, 106, 5, 127, 265, 99, 56, 201, 23, 103, 174, 287, 87, 230, 1, 49, 124, 173, 82, 215, 172, 288, 107, 271, 11, 108, 83, 136, 160, 198, 182, 217, 207, 88, 32, 257, 2, 233]\n",
      "Time Taken to generate binary vectors(k = 70): 2128.7754426002502 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [41, 33, 76, 279, 158, 144, 237, 244, 8, 19, 126, 100, 196, 93, 291, 245, 51, 247, 178, 243, 277, 96, 42, 118, 194, 36, 86, 21, 119, 107, 71, 166, 217, 269, 229, 184, 117, 115, 231, 141, 289, 282, 79, 12, 132, 241, 264, 32, 83, 124, 198, 169, 147, 159, 92, 154, 40, 105, 4, 288, 37, 67, 193, 38, 143, 252, 44, 157, 212, 220, 104, 283, 114, 258, 180, 185, 197, 227, 202, 27, 165, 224, 228, 218, 112, 275, 97, 1, 134, 215]\n",
      "Time Taken to generate binary vectors(k = 90): 2729.048449277878 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [17, 219, 215, 223, 298, 290, 33, 114, 85, 183, 67, 258, 208, 122, 281, 140, 294, 182, 100, 144, 292, 204, 131, 252, 160, 59, 125, 19, 201, 37, 177, 52, 280, 171, 14, 132, 112, 12, 247, 291, 3, 203, 22, 261, 192, 293, 227, 34, 23, 241, 118, 230, 272, 87, 187, 20, 83, 58, 188, 138, 41, 133, 297, 66, 256, 88, 103, 105, 169, 172, 94, 266, 174, 86, 69, 96, 221, 245, 278, 89, 121, 1, 285, 11, 40, 198, 287, 190, 15, 71, 176, 57, 250, 189, 236, 124, 137, 55, 143, 296, 82, 156, 60, 276, 141, 54, 167, 259, 154, 148]\n",
      "Time Taken to generate binary vectors(k = 110): 3328.7621116638184 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [194, 224, 222, 90, 158, 72, 293, 89, 140, 100, 28, 45, 68, 78, 274, 2, 206, 209, 242, 148, 129, 29, 298, 236, 56, 159, 81, 151, 26, 124, 123, 230, 168, 128, 92, 284, 157, 166, 121, 103, 256, 253, 235, 299, 86, 185, 50, 22, 133, 176, 277, 125, 74, 229, 126, 155, 228, 295, 30, 116, 276, 14, 55, 44, 83, 286, 127, 248, 212, 102, 184, 150, 135, 259, 297, 269, 193, 62, 202, 153, 205, 165, 82, 249, 73, 120, 271, 142, 138, 88, 6, 188, 265, 134, 266, 84, 41, 260, 52, 245, 93, 163, 111, 5, 280, 67, 220, 136, 15, 180, 213, 7, 196, 17, 48, 95, 227, 18, 240, 175, 64, 152, 31, 113, 3, 255, 10, 105, 263, 200]\n",
      "Time Taken to generate binary vectors(k = 130): 3929.8682622909546 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [200, 50, 213, 59, 212, 190, 275, 48, 293, 80, 260, 151, 280, 215, 218, 270, 68, 239, 160, 49, 86, 164, 145, 88, 154, 137, 243, 129, 252, 122, 29, 55, 6, 97, 138, 39, 2, 123, 284, 53, 207, 295, 274, 269, 208, 214, 155, 32, 89, 9, 199, 286, 19, 233, 82, 156, 21, 92, 105, 147, 265, 108, 264, 296, 7, 273, 90, 42, 238, 204, 245, 14, 205, 144, 254, 298, 244, 271, 234, 91, 255, 159, 266, 165, 111, 8, 58, 112, 249, 248, 223, 173, 294, 66, 95, 100, 61, 171, 12, 203, 78, 168, 23, 206, 128, 44, 290, 229, 13, 41, 194, 26, 140, 235, 153, 70, 170, 31, 37, 192, 291, 232, 136, 149, 299, 11, 191, 27, 30, 186, 195, 104, 1, 246, 118, 211, 121, 101, 297, 185, 179, 236, 15, 81, 257, 38, 259, 225, 289, 35]\n",
      "Time Taken to generate binary vectors(k = 150): 4527.362153530121 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [245, 25, 79, 187, 161, 197, 250, 205, 240, 195, 129, 214, 75, 82, 66, 119, 84, 169, 174, 39, 78, 112, 124, 57, 51, 88, 203, 157, 266, 24, 60, 61, 154, 131, 269, 190, 73, 291, 178, 93, 270, 160, 292, 247, 257, 52, 54, 22, 126, 275, 128, 246, 111, 12, 30, 207, 76, 172, 227, 192, 149, 74, 217, 244, 184, 299, 273, 232, 280, 223, 18, 55, 44, 115, 6, 237, 145, 254, 163, 0, 17, 294, 9, 81, 114, 89, 251, 239, 3, 21, 35, 225, 159, 36, 32, 13, 1, 220, 46, 140, 201, 92, 268, 11, 150, 177, 224, 121, 100, 42, 113, 105, 34, 144, 208, 165, 14, 53, 7, 233, 56, 285, 122, 238, 162, 59, 132, 106, 101, 27, 143, 277, 98, 236, 204, 116, 164, 183, 213, 80, 290, 146, 252, 230, 4, 2, 85, 108, 241, 286, 33, 198, 202, 210, 260, 8, 104, 287, 151, 175, 107, 86, 173, 284, 16, 48, 91, 37, 63, 152]\n",
      "Time Taken to generate binary vectors(k = 170): 5132.845893621445 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [176, 211, 298, 121, 252, 17, 195, 57, 129, 115, 208, 142, 155, 69, 250, 46, 39, 47, 266, 40, 19, 164, 76, 114, 9, 79, 161, 24, 20, 201, 143, 220, 102, 37, 33, 27, 95, 224, 290, 2, 205, 177, 158, 124, 284, 196, 94, 275, 296, 141, 200, 242, 62, 74, 179, 64, 207, 163, 170, 276, 13, 206, 10, 175, 191, 108, 32, 292, 209, 70, 98, 41, 4, 160, 58, 293, 286, 96, 210, 233, 218, 5, 12, 287, 133, 247, 186, 244, 230, 18, 126, 215, 100, 110, 197, 43, 234, 72, 216, 297, 80, 169, 295, 152, 136, 29, 146, 42, 277, 174, 166, 50, 282, 11, 63, 235, 138, 68, 178, 78, 232, 192, 294, 16, 257, 92, 147, 229, 45, 184, 1, 15, 236, 145, 3, 77, 173, 269, 246, 272, 263, 111, 172, 199, 90, 159, 0, 189, 144, 48, 54, 84, 60, 280, 71, 279, 83, 52, 223, 198, 65, 171, 291, 66, 168, 153, 61, 262, 231, 213, 157, 88, 30, 117, 249, 214, 148, 118, 273, 225, 238, 261, 44, 271, 154, 156, 248, 278, 109, 285]\n",
      "Time Taken to generate binary vectors(k = 190): 5724.967362880707 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [297, 284, 293, 82, 286, 215, 231, 203, 263, 95, 128, 153, 262, 31, 185, 139, 66, 217, 194, 160, 24, 266, 64, 47, 40, 259, 61, 145, 188, 22, 243, 256, 265, 21, 245, 219, 164, 52, 122, 36, 207, 86, 150, 23, 296, 183, 148, 26, 88, 63, 195, 270, 273, 200, 46, 288, 138, 78, 58, 99, 92, 271, 81, 121, 74, 11, 14, 199, 267, 30, 57, 201, 76, 142, 141, 260, 182, 51, 98, 115, 154, 227, 133, 55, 83, 32, 79, 167, 278, 102, 48, 44, 261, 97, 193, 39, 100, 275, 240, 232, 223, 140, 211, 184, 33, 91, 277, 236, 109, 253, 123, 257, 118, 125, 119, 28, 62, 295, 12, 213, 292, 90, 18, 230, 226, 43, 210, 19, 224, 87, 96, 250, 252, 130, 126, 151, 285, 298, 221, 247, 246, 156, 45, 10, 190, 116, 166, 282, 228, 163, 234, 68, 206, 187, 299, 110, 179, 254, 165, 7, 155, 152, 149, 0, 112, 120, 162, 202, 238, 280, 80, 177, 291, 272, 137, 269, 129, 59, 180, 274, 276, 132, 239, 71, 17, 294, 216, 229, 107, 197, 6, 94, 192, 20, 283, 8, 60, 159, 111, 255, 29, 287, 143, 258, 225, 77, 50, 49, 25, 124]\n",
      "Time Taken to generate binary vectors(k = 210): 6333.679386377335 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [287, 252, 243, 84, 127, 120, 217, 25, 275, 137, 51, 2, 68, 238, 278, 151, 6, 9, 105, 215, 175, 222, 135, 118, 206, 211, 44, 0, 288, 23, 212, 196, 255, 80, 128, 179, 290, 210, 149, 37, 246, 142, 292, 110, 63, 136, 231, 79, 32, 237, 38, 5, 219, 178, 281, 270, 261, 134, 41, 236, 190, 98, 113, 170, 35, 293, 119, 58, 14, 30, 221, 191, 40, 16, 198, 257, 209, 226, 26, 173, 145, 94, 262, 123, 284, 60, 266, 122, 264, 183, 21, 171, 283, 240, 103, 86, 20, 160, 188, 117, 121, 69, 85, 104, 250, 203, 169, 8, 263, 247, 230, 280, 147, 3, 207, 239, 72, 202, 124, 141, 193, 286, 144, 49, 162, 168, 274, 216, 291, 172, 297, 129, 13, 233, 36, 295, 106, 11, 273, 185, 116, 27, 17, 31, 213, 150, 48, 158, 224, 248, 299, 174, 139, 46, 34, 4, 186, 199, 184, 276, 166, 272, 77, 56, 109, 180, 165, 187, 33, 50, 65, 214, 138, 59, 140, 254, 268, 176, 154, 220, 12, 55, 7, 260, 39, 259, 64, 232, 22, 96, 47, 73, 155, 43, 189, 192, 267, 251, 201, 57, 1, 161, 227, 97, 204, 102, 269, 258, 245, 153, 28, 265, 62, 24, 218, 143, 289, 71, 298, 167, 101, 18, 78, 76, 242, 91, 42, 83, 100, 108]\n",
      "Time Taken to generate binary vectors(k = 230): 6928.7646379470825 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [36, 194, 185, 293, 245, 60, 173, 250, 297, 254, 274, 85, 291, 218, 80, 272, 150, 226, 251, 142, 269, 191, 157, 160, 125, 289, 262, 133, 122, 276, 24, 267, 156, 216, 260, 79, 43, 65, 183, 248, 2, 103, 204, 249, 223, 196, 21, 47, 268, 298, 294, 153, 234, 227, 49, 101, 12, 199, 81, 127, 110, 92, 113, 263, 27, 124, 282, 28, 66, 67, 265, 165, 246, 195, 215, 114, 278, 235, 104, 86, 5, 129, 98, 33, 18, 117, 261, 152, 155, 115, 175, 84, 184, 225, 100, 171, 239, 90, 154, 236, 52, 6, 213, 130, 257, 181, 45, 4, 210, 198, 40, 34, 82, 95, 148, 118, 35, 146, 123, 94, 151, 162, 89, 219, 136, 68, 141, 83, 158, 259, 147, 91, 252, 286, 41, 228, 78, 44, 168, 197, 106, 128, 138, 189, 8, 258, 182, 105, 140, 74, 211, 119, 167, 48, 170, 166, 121, 275, 135, 177, 15, 51, 1, 238, 25, 242, 270, 224, 233, 14, 88, 176, 179, 283, 220, 112, 159, 271, 143, 299, 22, 214, 186, 11, 295, 149, 55, 192, 292, 187, 56, 29, 17, 201, 212, 145, 281, 232, 58, 26, 209, 231, 285, 76, 273, 222, 190, 23, 253, 73, 207, 172, 87, 264, 46, 108, 169, 280, 163, 288, 77, 71, 279, 57, 296, 96, 180, 54, 61, 19, 255, 132, 217, 120, 109, 277, 16, 70, 178, 59, 97, 131, 7, 0, 93, 174, 193, 72, 205, 137]\n",
      "Time Taken to generate binary vectors(k = 250): 7533.202527761459 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [144, 190, 147, 93, 297, 86, 287, 140, 103, 172, 282, 5, 169, 187, 128, 92, 235, 221, 83, 223, 111, 77, 85, 206, 158, 114, 104, 175, 227, 26, 208, 271, 108, 192, 276, 191, 199, 50, 24, 165, 269, 2, 122, 149, 203, 89, 173, 167, 160, 195, 38, 219, 226, 275, 193, 244, 117, 4, 154, 64, 67, 17, 231, 151, 143, 253, 3, 6, 243, 119, 131, 100, 272, 84, 91, 135, 255, 141, 202, 71, 180, 148, 116, 9, 7, 261, 274, 99, 281, 178, 146, 29, 201, 142, 229, 267, 88, 283, 184, 75, 120, 258, 241, 20, 37, 299, 239, 138, 54, 49, 163, 78, 277, 51, 53, 80, 60, 251, 19, 228, 11, 57, 156, 293, 66, 45, 109, 279, 292, 290, 90, 225, 113, 183, 273, 177, 294, 55, 176, 0, 72, 186, 179, 98, 247, 232, 248, 222, 76, 263, 22, 82, 105, 129, 264, 97, 27, 288, 69, 224, 1, 127, 291, 220, 256, 107, 252, 298, 200, 262, 130, 286, 211, 215, 95, 106, 121, 123, 102, 14, 23, 32, 268, 70, 259, 152, 137, 159, 194, 132, 242, 153, 218, 68, 41, 48, 237, 62, 185, 238, 110, 150, 101, 96, 74, 188, 16, 157, 245, 59, 79, 289, 162, 280, 58, 125, 171, 36, 233, 46, 25, 205, 170, 270, 145, 182, 21, 139, 40, 61, 112, 161, 15, 18, 257, 250, 33, 265, 216, 12, 10, 73, 212, 8, 295, 34, 210, 35, 63, 230, 28, 214, 136, 213, 296, 266, 278, 284, 87, 236, 189, 249, 65, 260, 217, 164, 13, 124, 52, 204]\n",
      "Time Taken to generate binary vectors(k = 270): 8222.080114841461 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n",
      "Index: [27, 5, 214, 222, 224, 2, 245, 285, 191, 246, 184, 35, 210, 269, 264, 227, 177, 172, 114, 244, 225, 211, 202, 295, 121, 230, 215, 139, 44, 132, 89, 9, 148, 103, 43, 13, 175, 155, 223, 67, 135, 76, 99, 49, 152, 163, 267, 178, 26, 66, 257, 119, 56, 136, 91, 190, 187, 75, 188, 259, 131, 143, 78, 123, 120, 176, 255, 294, 22, 149, 162, 29, 166, 54, 32, 156, 287, 109, 278, 85, 69, 256, 238, 73, 265, 234, 141, 241, 198, 142, 286, 52, 107, 77, 129, 251, 127, 186, 57, 212, 96, 167, 101, 146, 102, 170, 45, 272, 83, 203, 160, 242, 95, 154, 159, 137, 209, 104, 100, 30, 0, 68, 217, 1, 266, 15, 282, 165, 23, 125, 206, 50, 71, 122, 168, 270, 144, 213, 25, 196, 158, 62, 235, 118, 20, 237, 128, 293, 60, 233, 40, 218, 173, 105, 37, 124, 151, 220, 231, 110, 181, 8, 117, 31, 24, 94, 72, 47, 111, 157, 185, 90, 33, 252, 61, 84, 219, 290, 88, 277, 221, 254, 130, 11, 80, 74, 268, 289, 36, 194, 4, 97, 34, 279, 115, 283, 28, 82, 204, 299, 64, 195, 19, 297, 161, 182, 70, 273, 106, 281, 7, 63, 134, 79, 39, 150, 10, 53, 298, 171, 262, 46, 108, 174, 58, 296, 280, 147, 98, 236, 200, 260, 288, 145, 248, 140, 133, 3, 48, 164, 179, 291, 65, 253, 113, 169, 271, 232, 243, 263, 284, 138, 189, 41, 51, 14, 275, 6, 199, 21, 192, 93, 207, 87, 258, 180, 81, 38, 247, 16, 292, 59, 112, 42, 250, 201, 249, 183, 17, 116, 239, 92, 274, 226, 229, 193, 197, 228, 216, 18]\n",
      "Time Taken to generate binary vectors(k = 290): 8821.674087047577 secs\n",
      "Binary vectors successfully added, Exporting to Json...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "# Import Hash functions *\n",
    "random_hash_functions_df = pd.read_json('LSH Search Results/LSH 300 Random Hash Functions.json')\n",
    "print(\"Imported  hash functions from Json file\")\n",
    "# df['Vector'] = df['Vector'].apply(lambda x : json.loads(x))\n",
    "\n",
    "for i in range(10,300,20):\n",
    "    start_time = time.time()\n",
    "    i_size_random_hash_functions = random_hash_functions_df.sample(i)\n",
    "    print(\"Index:\",list(i_size_random_hash_functions.index))\n",
    "    # # Do LSH Calculations for dataset *\n",
    "    df['Binary Vectors'] = df['Vector'].apply( lambda x : generate_binary_vectors(x,i_size_random_hash_functions))\n",
    "    end_time = time.time()\n",
    "    print(\"Time Taken to generate binary vectors(k =\",str(i)+\"):\", end_time-start_time, \"secs\")\n",
    "    print(\"Binary vectors successfully added, Exporting to Json...\")\n",
    "    df.to_csv('LSH Search Results/LSH Database with Binary Vectors (k is '+str(i)+').csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate 10 different queries accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to generate binary vectors: 0.18794512748718262\n",
      "Time Taken to generate binary vectors: 0.1875159740447998\n",
      "Time Taken to generate binary vectors: 0.18975400924682617\n",
      "Time Taken to generate binary vectors: 0.18224525451660156\n",
      "Time Taken to generate binary vectors: 0.17214012145996094\n",
      "Time Taken to generate binary vectors: 0.20138168334960938\n",
      "Time Taken to generate binary vectors: 0.180006742477417\n",
      "Time Taken to generate binary vectors: 0.1985175609588623\n",
      "Time Taken to generate binary vectors: 0.19063329696655273\n",
      "Time Taken to generate binary vectors: 0.19053316116333008\n"
     ]
    }
   ],
   "source": [
    "# Calculate 10 different queries accuracies\n",
    "import os\n",
    "\n",
    "# Defining queries\n",
    "query_dict = {'Q1': \"resilient investment banker\", \n",
    "              'Q2': \"2 years experience product manager\", \n",
    "              'Q3': \"10 years risk analyst problem solver\", \n",
    "              'Q4': \"tax analyst for big company\", \n",
    "              'Q5': \"software engineer for google or amazon\", \n",
    "              'Q6': \"video editor for advertisements with 5 year experience\",\n",
    "              'Q7': \"full time senior head nurse position\",\n",
    "              'Q8': \"after school math and science tutor\",\n",
    "              'Q9': \"dietitian for professional atheletes\",\n",
    "              'Q10': \"costume designer and makeup artist\"}\n",
    "\n",
    "query_df = pd.DataFrame.from_dict(query_dict, orient='index', columns=['Query Text'])\n",
    "\n",
    "# Using the imported count vectorizer to generate counter vector of queries (N)\n",
    "query_vectors = vectorizer.transform(list(query_dict.values()))\n",
    "query_matrix = query_vectors.toarray()\n",
    "\n",
    "# Import Hash functions *\n",
    "random_hash_functions_df = pd.read_json('LSH Search Results/LSH Random Hash Functions.json')\n",
    "\n",
    "# Do LSH Calculations for query vectors *\n",
    "query_dict_binary_vectors = [generate_binary_vectors(vector, random_hash_functions_df) for vector in query_matrix]\n",
    "\n",
    "query_df['Vectors'] = [row.tolist() for row in query_matrix]\n",
    "query_df['Binary Vectors'] = query_dict_binary_vectors\n",
    "\n",
    "query_df.to_csv('LSH Search Results/LSH Query Vectors with Binary Vectors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragne\\AppData\\Local\\Temp\\ipykernel_22204\\1163304622.py:36: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(len(query_df['Binary Vectors'][0]))\n"
     ]
    }
   ],
   "source": [
    "# Calculate 10 different queries accuracies\n",
    "import os\n",
    "\n",
    "# Defining queries\n",
    "query_dict = {'Q1': \"resilient investment banker\", \n",
    "              'Q2': \"2 years experience product manager\", \n",
    "              'Q3': \"10 years risk analyst problem solver\", \n",
    "              'Q4': \"tax analyst for big company\", \n",
    "              'Q5': \"software engineer for google or amazon\", \n",
    "              'Q6': \"video editor for advertisements with 5 year experience\",\n",
    "              'Q7': \"full time senior head nurse position\",\n",
    "              'Q8': \"after school math and science tutor\",\n",
    "              'Q9': \"dietitian for professional atheletes\",\n",
    "              'Q10': \"costume designer and makeup artist\"}\n",
    "\n",
    "query_df = pd.DataFrame.from_dict(query_dict, orient='index', columns=['Query Text'])\n",
    "\n",
    "# Using the imported count vectorizer to generate counter vector of queries (N)\n",
    "query_vectors = vectorizer.transform(list(query_dict.values()))\n",
    "query_matrix = query_vectors.toarray()\n",
    "\n",
    "\n",
    "# Import Hash functions *\n",
    "index = [27, 5, 214, 222, 224, 2, 245, 285, 191, 246, 184, 35, 210, 269, 264, 227, 177, 172, 114, 244, 225, 211, 202, 295, 121, 230, 215, 139, 44, 132, 89, 9, 148, 103, 43, 13, 175, 155, 223, 67, 135, 76, 99, 49, 152, 163, 267, 178, 26, 66, 257, 119, 56, 136, 91, 190, 187, 75, 188, 259, 131, 143, 78, 123, 120, 176, 255, 294, 22, 149, 162, 29, 166, 54, 32, 156, 287, 109, 278, 85, 69, 256, 238, 73, 265, 234, 141, 241, 198, 142, 286, 52, 107, 77, 129, 251, 127, 186, 57, 212, 96, 167, 101, 146, 102, 170, 45, 272, 83, 203, 160, 242, 95, 154, 159, 137, 209, 104, 100, 30, 0, 68, 217, 1, 266, 15, 282, 165, 23, 125, 206, 50, 71, 122, 168, 270, 144, 213, 25, 196, 158, 62, 235, 118, 20, 237, 128, 293, 60, 233, 40, 218, 173, 105, 37, 124, 151, 220, 231, 110, 181, 8, 117, 31, 24, 94, 72, 47, 111, 157, 185, 90, 33, 252, 61, 84, 219, 290, 88, 277, 221, 254, 130, 11, 80, 74, 268, 289, 36, 194, 4, 97, 34, 279, 115, 283, 28, 82, 204, 299, 64, 195, 19, 297, 161, 182, 70, 273, 106, 281, 7, 63, 134, 79, 39, 150, 10, 53, 298, 171, 262, 46, 108, 174, 58, 296, 280, 147, 98, 236, 200, 260, 288, 145, 248, 140, 133, 3, 48, 164, 179, 291, 65, 253, 113, 169, 271, 232, 243, 263, 284, 138, 189, 41, 51, 14, 275, 6, 199, 21, 192, 93, 207, 87, 258, 180, 81, 38, 247, 16, 292, 59, 112, 42, 250, 201, 249, 183, 17, 116, 239, 92, 274, 226, 229, 193, 197, 228, 216, 18]\n",
    "k = len(index)\n",
    "random_hash_functions_df = pd.read_json('LSH Search Results/LSH 300 Random Hash Functions.json').iloc[index]\n",
    "\n",
    "# Do LSH Calculations for query vectors *\n",
    "query_dict_binary_vectors = [generate_binary_vectors(vector, random_hash_functions_df) for vector in query_matrix]\n",
    "\n",
    "query_df['Vectors'] = [row.tolist() for row in query_matrix]\n",
    "query_df['Binary Vectors'] = query_dict_binary_vectors\n",
    "\n",
    "filename = f'LSH Search Results\\LSH Query Vector With Binary Vectors\\LSH Query Vectors with Binary Vectors(k={k}).csv'\n",
    "query_df.to_csv(filename)\n",
    "print(len(query_df['Binary Vectors'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query Text</th>\n",
       "      <th>Vectors</th>\n",
       "      <th>Binary Vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q1</th>\n",
       "      <td>resilient investment banker</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 1, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q2</th>\n",
       "      <td>2 years experience product manager</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 1, 1, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q3</th>\n",
       "      <td>10 years risk analyst problem solver</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4</th>\n",
       "      <td>tax analyst for big company</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q5</th>\n",
       "      <td>software engineer for google or amazon</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 1, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q6</th>\n",
       "      <td>video editor for advertisements with 5 year ex...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 1, 1, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q7</th>\n",
       "      <td>full time senior head nurse position</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q8</th>\n",
       "      <td>after school math and science tutor</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q9</th>\n",
       "      <td>dietitian for professional atheletes</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 1, 1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q10</th>\n",
       "      <td>costume designer and makeup artist</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Query Text  \\\n",
       "Q1                         resilient investment banker   \n",
       "Q2                  2 years experience product manager   \n",
       "Q3                10 years risk analyst problem solver   \n",
       "Q4                         tax analyst for big company   \n",
       "Q5              software engineer for google or amazon   \n",
       "Q6   video editor for advertisements with 5 year ex...   \n",
       "Q7                full time senior head nurse position   \n",
       "Q8                 after school math and science tutor   \n",
       "Q9                dietitian for professional atheletes   \n",
       "Q10                 costume designer and makeup artist   \n",
       "\n",
       "                                               Vectors  \\\n",
       "Q1   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "Q2   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "Q3   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "Q4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "Q5   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "Q6   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "Q7   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "Q8   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "Q9   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "Q10  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                     Binary Vectors  \n",
       "Q1   [1, 1, 0, 0, 0, 1, 1, 1, 1, 0]  \n",
       "Q2   [1, 1, 1, 0, 0, 1, 1, 1, 0, 0]  \n",
       "Q3   [1, 1, 1, 0, 1, 0, 0, 1, 0, 0]  \n",
       "Q4   [1, 1, 0, 0, 1, 1, 1, 1, 1, 1]  \n",
       "Q5   [0, 0, 0, 0, 1, 1, 1, 1, 0, 0]  \n",
       "Q6   [1, 1, 0, 0, 1, 1, 1, 1, 1, 0]  \n",
       "Q7   [1, 0, 0, 0, 1, 1, 0, 0, 0, 0]  \n",
       "Q8   [0, 1, 0, 0, 0, 0, 1, 1, 0, 1]  \n",
       "Q9   [0, 1, 0, 0, 0, 1, 1, 1, 0, 1]  \n",
       "Q10  [0, 0, 0, 0, 0, 1, 1, 1, 1, 0]  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hash_Functions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0652451414, -0.3105812535, 0.1707870626, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-1.3519581269, -0.2255419904, 0.1975276205, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.7905945027, -0.7924027689, 0.9140652319, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.2492531758, 2.1408192351, -1.5438940171, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.5068522531, -0.43001041840000004, -1.153862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>[0.3290855109, -0.18241482, 0.9899872848000001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>[1.0772217896, -0.2121105697, 1.1417691648, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>[2.1115217074, 0.5449863267, 0.7089783419, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>[-0.15233194590000002, 1.6498596557, 2.5377461...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>[-0.08525320210000001, 0.9760665148000001, -1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Hash_Functions\n",
       "0    [0.0652451414, -0.3105812535, 0.1707870626, -0...\n",
       "1    [-1.3519581269, -0.2255419904, 0.1975276205, -...\n",
       "2    [0.7905945027, -0.7924027689, 0.9140652319, -0...\n",
       "3    [-0.2492531758, 2.1408192351, -1.5438940171, -...\n",
       "4    [1.5068522531, -0.43001041840000004, -1.153862...\n",
       "..                                                 ...\n",
       "295  [0.3290855109, -0.18241482, 0.9899872848000001...\n",
       "296  [1.0772217896, -0.2121105697, 1.1417691648, -0...\n",
       "297  [2.1115217074, 0.5449863267, 0.7089783419, -0....\n",
       "298  [-0.15233194590000002, 1.6498596557, 2.5377461...\n",
       "299  [-0.08525320210000001, 0.9760665148000001, -1....\n",
       "\n",
       "[300 rows x 1 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_hash_functions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "k=30\n",
    "query_df = pd.read_csv(f'LSH Search Results/LSH Query Vectors with Binary Vectors(k={k}).csv')\n",
    "df = pd.read_csv(f'LSH Search Results/LSH Database with Binary Vectors (k is {k}).csv')\n",
    "\n",
    "query_df['Binary Vectors'] = query_df['Binary Vectors'].apply(lambda x : json.loads(x))\n",
    "df['Binary Vectors'] = df['Binary Vectors'].apply(lambda x : json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(query_df['Binary Vectors'][0]))\n",
    "print(len(df['Binary Vectors'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resilient investment banker'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_df['Query Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration taken to calculate hamming distance of '0' with dataset: 1.5441560745239258\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.6484711170196533\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.6557807922363281\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.6359128952026367\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.646578311920166\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.6443042755126953\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.6614692211151123\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.6201503276824951\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.648831844329834\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.6394858360290527\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 1.2146499156951904\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.6847889423370361\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.6525154113769531\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.6566476821899414\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.6482405662536621\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.6538503170013428\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.6716048717498779\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.6659078598022461\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.660015344619751\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.6687908172607422\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 1.4537632465362549\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.7301547527313232\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.6930794715881348\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.6889760494232178\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.6873395442962646\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.6727781295776367\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.6962239742279053\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.6693799495697021\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.6836211681365967\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.7053241729736328\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 1.6246387958526611\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.6769530773162842\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.6774680614471436\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.6811740398406982\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.6907551288604736\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.6820230484008789\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.6846446990966797\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.6855804920196533\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.7019503116607666\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.6803255081176758\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 1.7406516075134277\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.8133926391601562\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.7097434997558594\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.7142772674560547\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.7058746814727783\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.7103047370910645\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.7250761985778809\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.6937780380249023\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.6881017684936523\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.707921028137207\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 1.7637691497802734\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.7229750156402588\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.7033910751342773\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.7217953205108643\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.7407166957855225\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.7221205234527588\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.7188878059387207\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.7190425395965576\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.7139794826507568\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.7177228927612305\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 1.9378767013549805\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.7304930686950684\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.7452735900878906\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.7233471870422363\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.7372746467590332\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.7206687927246094\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.742387056350708\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.7378315925598145\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.7333166599273682\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.7319457530975342\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 2.0298798084259033\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.7445821762084961\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.7281122207641602\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.7277781963348389\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.7448000907897949\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.745582103729248\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.747678279876709\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.7350783348083496\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.7486348152160645\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.7400078773498535\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 2.0697708129882812\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.7581591606140137\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.7687091827392578\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.7535703182220459\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.7796285152435303\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.7751514911651611\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.766110897064209\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.7602829933166504\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.763873815536499\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.7570383548736572\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 2.1227822303771973\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.7789294719696045\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.7853844165802002\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.7875509262084961\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.7769238948822021\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.7666256427764893\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.776907205581665\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.782372236251831\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.7688941955566406\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.7811262607574463\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 2.3173041343688965\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.7895400524139404\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.8019239902496338\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.7867374420166016\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.8069238662719727\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.8007135391235352\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.7927892208099365\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.8236160278320312\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.7904853820800781\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.7961277961730957\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 2.3394763469696045\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.8131141662597656\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.8132755756378174\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.8166601657867432\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.813422441482544\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.8324875831604004\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.8086652755737305\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.8081555366516113\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.8486239910125732\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.8206470012664795\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 2.390195846557617\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.8232917785644531\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.8264310359954834\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.8455202579498291\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.8193824291229248\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.8166754245758057\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.8217551708221436\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.8273637294769287\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.8241429328918457\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.8265318870544434\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 2.6783077716827393\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.8464910984039307\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.8359363079071045\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.8549017906188965\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.846177339553833\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.8446097373962402\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.8501639366149902\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.855870246887207\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.8347549438476562\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.8379197120666504\n",
      "Duration taken to calculate hamming distance of '0' with dataset: 2.5830886363983154\n",
      "Duration taken to calculate hamming distance of '1' with dataset: 0.8541443347930908\n",
      "Duration taken to calculate hamming distance of '2' with dataset: 0.8710365295410156\n",
      "Duration taken to calculate hamming distance of '3' with dataset: 0.8602774143218994\n",
      "Duration taken to calculate hamming distance of '4' with dataset: 0.8845663070678711\n",
      "Duration taken to calculate hamming distance of '5' with dataset: 0.8527035713195801\n",
      "Duration taken to calculate hamming distance of '6' with dataset: 0.858107328414917\n",
      "Duration taken to calculate hamming distance of '7' with dataset: 0.8668475151062012\n",
      "Duration taken to calculate hamming distance of '8' with dataset: 0.8653035163879395\n",
      "Duration taken to calculate hamming distance of '9' with dataset: 0.877129077911377\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "for k in range(10,300,20):\n",
    "    query_df = pd.read_csv(f'LSH Search Results\\LSH Query Vector With Binary Vectors\\LSH Query Vectors with Binary Vectors(k={k}).csv')\n",
    "    df = pd.read_csv(f'D:\\Data Analysis\\LSH Database with Binary Vectors\\LSH Database with Binary Vectors (k is {k}).csv')\n",
    "\n",
    "    query_df['Binary Vectors'] = query_df['Binary Vectors'].apply(lambda x : json.loads(x))\n",
    "    df['Binary Vectors'] = df['Binary Vectors'].apply(lambda x : json.loads(x))\n",
    "        \n",
    "    for query_index,query_row in query_df.iterrows():\n",
    "\n",
    "        # Specify the folder path\n",
    "        # folder_path = f'LSH Search Results\\LSH Search Results with K Values'\n",
    "\n",
    "        mismatched_list = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        for i, row in df.iterrows():\n",
    "            mismatched_list.append(hamming_distance(query_row['Binary Vectors'],row['Binary Vectors'])) \n",
    "        end_time = time.time()\n",
    "        print(\"Duration taken to calculate hamming distance of '\"+ str(query_index) +\"' with dataset:\", end_time-start_time)\n",
    "        \n",
    "        df['Distance'] = mismatched_list\n",
    "\n",
    "        # Get only data and distance from query_name\n",
    "        query_result_df = df[['Data', 'Distance']].copy()\n",
    "\n",
    "        filename = f'LSH Search Results\\LSH Search Results with K Values\\LSH Search Results (k = {k})(Query = {query_df[\"Query Text\"][query_index]}).csv'\n",
    "        query_result_df.to_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
