{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_json(\"job_postings_prepped.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Data  \\\n",
      "0  Licensed Insurance Agent While many industries...   \n",
      "1  Sales Manager Are dynamic creative marketing p...   \n",
      "2  Model Risk Auditor Join Us Model Risk Auditor ...   \n",
      "3  Business Manager Business ManagerFirst Baptist...   \n",
      "4  NY Studio Assistant YOU COULD BE ONE OF THE MA...   \n",
      "\n",
      "                                              Vector  \n",
      "0  [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "Dimension of the Vector:  105248\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration\n",
    "# print(df.info())\n",
    "print(df.head())\n",
    "# print(df.shape)\n",
    "\n",
    "# Data Analysis\n",
    "print(\"Dimension of the Vector: \", len(df['Vector'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Memory Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Dimension:  105248\n",
      "Vector Storage Size (MB):\n",
      "Index         0.625137\n",
      "Data         86.682539\n",
      "Vector    13651.414261\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "vector_dimension = len(df['Vector'][0])\n",
    "vector_size = df.memory_usage(deep=True)['Vector'] \n",
    "print(\"Vector Dimension: \", vector_dimension)\n",
    "print(\"Vector Storage Size (MB):\")\n",
    "print( df.memory_usage(deep=True) / 1024**2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Quantization \n",
    "Product Quatization is a method to reduce the memory usage of the codebook.\n",
    "The idea is to split the codebook into sub-codebooks and quantize the data into sub-vectors.\n",
    "The sub-vectors are then quantized into sub-codebooks.\n",
    "The sub-codebook indices are then concatenated to form the final codebook index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import nanopq \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "The data required is preprocessed and stored in a file. To use the data, loading the file is done to prevent unnecessary preprocessing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Type:  <class 'list'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape:  (15885, 105248)\n",
      "X Vector Type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries vectorizer\n",
    "\n",
    "# vectors data type\n",
    "print(\"Vector Type: \", type(df['Vector'][0]))\n",
    "X = np.array(df['Vector'].values.tolist(), dtype=np.float32)\n",
    "\n",
    "# Save the X vector\n",
    "np.save(\"PQ/X.npy\", X)\n",
    "\n",
    "print(\"X Shape: \", X.shape)\n",
    "print(\"X Vector Type: \", type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the vectors with 8 sub-spaces....\n",
      "M: 8, Ks: 8, metric : <class 'numpy.uint8'>, code_dtype: l2\n",
      "iter: 20, seed: 123\n",
      "Training the subspace: 0 / 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Encode the vectors\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEncoding the vectors with 8 sub-spaces....\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m pq_8_8_code, pq_8_8 \u001b[39m=\u001b[39m train_fit_pq(M\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, Ks\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, X\u001b[39m=\u001b[39;49mX)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m pq_8_16_code, pq_8_16 \u001b[39m=\u001b[39m train_fit_pq(M\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, Ks\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, X\u001b[39m=\u001b[39mX)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m pq_8_32_code, pq_8_32 \u001b[39m=\u001b[39m train_fit_pq(M\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, Ks\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, X\u001b[39m=\u001b[39mX)\n",
      "\u001b[1;32m/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb Cell 11\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pq \u001b[39m=\u001b[39m nanopq\u001b[39m.\u001b[39mPQ(M\u001b[39m=\u001b[39mM, Ks\u001b[39m=\u001b[39mKs) \u001b[39m# M=5 as vector dimension must be divisible by M\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Fit the data and Encode the vectors\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pq\u001b[39m.\u001b[39;49mfit(vecs\u001b[39m=\u001b[39;49mX)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m X_code \u001b[39m=\u001b[39m pq\u001b[39m.\u001b[39mencode(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m X_code, pq\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/nanopq/pq.py:123\u001b[0m, in \u001b[0;36mPQ.fit\u001b[0;34m(self, vecs, iter, seed, minit)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining the subspace: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mM))\n\u001b[1;32m    122\u001b[0m     vecs_sub \u001b[39m=\u001b[39m vecs[:, m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDs : (m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDs]\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcodewords[m], _ \u001b[39m=\u001b[39m kmeans2(vecs_sub, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mKs, \u001b[39miter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39miter\u001b[39;49m, minit\u001b[39m=\u001b[39;49mminit)\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/scipy/cluster/vq.py:786\u001b[0m, in \u001b[0;36mkmeans2\u001b[0;34m(data, k, iter, thresh, minit, missing, check_finite, seed)\u001b[0m\n\u001b[1;32m    782\u001b[0m         code_book \u001b[39m=\u001b[39m init_meth(data, k, rng)\n\u001b[1;32m    784\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39miter\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[39m# Compute the nearest neighbor for each obs using the current code book\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m     label \u001b[39m=\u001b[39m vq(data, code_book)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    787\u001b[0m     \u001b[39m# Update the code book by computing centroids\u001b[39;00m\n\u001b[1;32m    788\u001b[0m     new_code_book, has_members \u001b[39m=\u001b[39m _vq\u001b[39m.\u001b[39mupdate_cluster_means(data, label, nc)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/scipy/cluster/vq.py:201\u001b[0m, in \u001b[0;36mvq\u001b[0;34m(obs, code_book, check_finite)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvq\u001b[39m(obs, code_book, check_finite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    144\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39m    Assign codes from a code book to observations.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \n\u001b[1;32m    200\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     obs \u001b[39m=\u001b[39m _asarray_validated(obs, check_finite\u001b[39m=\u001b[39;49mcheck_finite)\n\u001b[1;32m    202\u001b[0m     code_book \u001b[39m=\u001b[39m _asarray_validated(code_book, check_finite\u001b[39m=\u001b[39mcheck_finite)\n\u001b[1;32m    203\u001b[0m     ct \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcommon_type(obs, code_book)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/scipy/_lib/_util.py:252\u001b[0m, in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mmasked arrays are not supported\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    251\u001b[0m toarray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray_chkfinite \u001b[39mif\u001b[39;00m check_finite \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39masarray\n\u001b[0;32m--> 252\u001b[0m a \u001b[39m=\u001b[39m toarray(a)\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m objects_ok:\n\u001b[1;32m    254\u001b[0m     \u001b[39mif\u001b[39;00m a\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mdtype(\u001b[39m'\u001b[39m\u001b[39mO\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/lib/function_base.py:627\u001b[0m, in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Convert the input to an array, checking for NaNs or Infs.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \n\u001b[1;32m    566\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m \n\u001b[1;32m    625\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    626\u001b[0m a \u001b[39m=\u001b[39m asarray(a, dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39morder)\n\u001b[0;32m--> 627\u001b[0m \u001b[39mif\u001b[39;00m a\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mchar \u001b[39min\u001b[39;00m typecodes[\u001b[39m'\u001b[39m\u001b[39mAllFloat\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39;49misfinite(a)\u001b[39m.\u001b[39mall():\n\u001b[1;32m    628\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    629\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39marray must not contain infs or NaNs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    630\u001b[0m \u001b[39mreturn\u001b[39;00m a\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def train_fit_pq(M, Ks, X):\n",
    "    # Initialize the PQ encoder (M=5 sub-spaces, 256 centroids per sub-space)\n",
    "    pq = nanopq.PQ(M=M, Ks=Ks) # M=5 as vector dimension must be divisible by M\n",
    "\n",
    "    # Fit the data and Encode the vectors\n",
    "    pq.fit(vecs=X)\n",
    "    X_code = pq.encode(X)\n",
    "    return X_code, pq\n",
    "\n",
    "# Encode the vectors\n",
    "print(\"Encoding the vectors with 8 sub-spaces....\")\n",
    "pq_8_8_code, pq_8_8 = train_fit_pq(M=8, Ks=8, X=X)\n",
    "pq_8_16_code, pq_8_16 = train_fit_pq(M=8, Ks=16, X=X)\n",
    "pq_8_32_code, pq_8_32 = train_fit_pq(M=8, Ks=32, X=X)\n",
    "pq_8_64_code, pq_8_64 = train_fit_pq(M=8, Ks=64, X=X)\n",
    "pq_8_128_code, pq_8_128 = train_fit_pq(M=8, Ks=128, X=X)\n",
    "\n",
    "print(\"Saving the PQ objects....\")\n",
    "# Save the X_code to a file\n",
    "np.save(\"PQ/pq_8_8_code.npy\", pq_8_8_code)\n",
    "np.save(\"PQ/pq_8_16_code.npy\", pq_8_16_code)\n",
    "np.save(\"PQ/pq_8_32_code.npy\", pq_8_32_code)\n",
    "np.save(\"PQ/pq_8_64_code.npy\", pq_8_64_code)\n",
    "np.save(\"PQ/pq_8_128_code.npy\", pq_8_128_code)\n",
    "\n",
    "# Save the pq object to a file\n",
    "pickle.dump(pq_8_8, open(\"PQ/pq_8_8.pkl\", \"wb\"))\n",
    "pickle.dump(pq_8_16, open(\"PQ/pq_8_16.pkl\", \"wb\"))\n",
    "pickle.dump(pq_8_32, open(\"PQ/pq_8_32.pkl\", \"wb\"))\n",
    "pickle.dump(pq_8_64, open(\"PQ/pq_8_64.pkl\", \"wb\"))\n",
    "pickle.dump(pq_8_128, open(\"PQ/pq_8_128.pkl\", \"wb\"))\n",
    "# ===============================================================\n",
    "print(\"Encoding the vectors with 16 sub-spaces....\")\n",
    "pq_16_8_code, pq_16_8 = train_fit_pq(M=16, Ks=8, X=X)\n",
    "pq_16_16_code, pq_16_16 = train_fit_pq(M=16, Ks=16, X=X)\n",
    "pq_16_32_code, pq_16_32 = train_fit_pq(M=16, Ks=32, X=X)\n",
    "pq_16_64_code, pq_16_64 = train_fit_pq(M=16, Ks=64, X=X)\n",
    "pq_16_128_code, pq_16_128 = train_fit_pq(M=16, Ks=128, X=X)\n",
    "\n",
    "np.save(\"PQ/pq_16_8_code.npy\", pq_16_8_code)\n",
    "np.save(\"PQ/pq_16_16_code.npy\", pq_16_16_code)\n",
    "np.save(\"PQ/pq_16_32_code.npy\", pq_16_32_code)\n",
    "np.save(\"PQ/pq_16_64_code.npy\", pq_16_64_code)\n",
    "np.save(\"PQ/pq_16_128_code.npy\", pq_16_128_code)\n",
    "\n",
    "pickle.dump(pq_16_8, open(\"PQ/pq_16_8.pkl\", \"wb\"))\n",
    "pickle.dump(pq_16_16, open(\"PQ/pq_16_16.pkl\", \"wb\"))\n",
    "pickle.dump(pq_16_32, open(\"PQ/pq_16_32.pkl\", \"wb\"))\n",
    "pickle.dump(pq_16_64, open(\"PQ/pq_16_64.pkl\", \"wb\"))\n",
    "pickle.dump(pq_16_128, open(\"PQ/pq_16_128.pkl\", \"wb\"))\n",
    "# ===============================================================\n",
    "\n",
    "print(\"Encoding the vectors with 32 sub-spaces....\")\n",
    "pq_32_8_code, pq_32_8 = train_fit_pq(M=32, Ks=8, X=X)\n",
    "pq_32_16_code, pq_32_16 = train_fit_pq(M=32, Ks=16, X=X)\n",
    "pq_32_32_code, pq_32_32 = train_fit_pq(M=32, Ks=32, X=X)\n",
    "pq_32_64_code, pq_32_64 = train_fit_pq(M=32, Ks=64, X=X)\n",
    "pq_32_128_code, pq_32_128 = train_fit_pq(M=32, Ks=128, X=X)\n",
    "\n",
    "np.save(\"PQ/pq_32_8_code.npy\", pq_32_8_code)\n",
    "np.save(\"PQ/pq_32_16_code.npy\", pq_32_16_code)\n",
    "np.save(\"PQ/pq_32_32_code.npy\", pq_32_32_code)\n",
    "np.save(\"PQ/pq_32_64_code.npy\", pq_32_64_code)\n",
    "np.save(\"PQ/pq_32_128_code.npy\", pq_32_128_code)\n",
    "\n",
    "pickle.dump(pq_32_8, open(\"PQ/pq_32_8.pkl\", \"wb\"))\n",
    "pickle.dump(pq_32_16, open(\"PQ/pq_32_16.pkl\", \"wb\"))\n",
    "pickle.dump(pq_32_32, open(\"PQ/pq_32_32.pkl\", \"wb\"))\n",
    "pickle.dump(pq_32_64, open(\"PQ/pq_32_64.pkl\", \"wb\"))\n",
    "pickle.dump(pq_32_128, open(\"PQ/pq_32_128.pkl\", \"wb\"))\n",
    "\n",
    "# ===============================================================\n",
    "\n",
    "# print(\"Encoding the vectors with 64 sub-spaces....\")\n",
    "# pq_64_8_code, pq_64_8 = train_fit_pq(M=64, Ks=8, X=X)\n",
    "# pq_64_16_code, pq_64_16 = train_fit_pq(M=64, Ks=16, X=X)\n",
    "# pq_64_32_code, pq_64_32 = train_fit_pq(M=64, Ks=32, X=X)\n",
    "# pq_64_64_code, pq_64_64 = train_fit_pq(M=64, Ks=64, X=X)\n",
    "# pq_64_128_code, pq_64_128 = train_fit_pq(M=64, Ks=128, X=X)\n",
    "\n",
    "# np.save(\"PQ/pq_64_8_code.npy\", pq_64_8_code)\n",
    "# np.save(\"PQ/pq_64_16_code.npy\", pq_64_16_code)\n",
    "# np.save(\"PQ/pq_64_32_code.npy\", pq_64_32_code)\n",
    "# np.save(\"PQ/pq_64_64_code.npy\", pq_64_64_code)\n",
    "# np.save(\"PQ/pq_64_128_code.npy\", pq_64_128_code)\n",
    "\n",
    "# pickle.dump(pq_64_8, open(\"PQ/pq_64_8.pkl\", \"wb\"))\n",
    "# pickle.dump(pq_64_16, open(\"PQ/pq_64_16.pkl\", \"wb\"))\n",
    "# pickle.dump(pq_64_32, open(\"PQ/pq_64_32.pkl\", \"wb\"))\n",
    "# pickle.dump(pq_64_64, open(\"PQ/pq_64_64.pkl\", \"wb\"))\n",
    "# pickle.dump(pq_64_128, open(\"PQ/pq_64_128.pkl\", \"wb\"))\n",
    "\n",
    "# ===============================================================\n",
    "\n",
    "# print(\"Encoding the vectors with 128 sub-spaces....\")\n",
    "# pq_128_8_code, pq_128_8 = train_fit_pq(M=128, Ks=8, X=X)\n",
    "# pq_128_16_code, pq_128_16 = train_fit_pq(M=128, Ks=16, X=X)\n",
    "# pq_128_32_code, pq_128_32 = train_fit_pq(M=128, Ks=32, X=X)\n",
    "# pq_128_64_code, pq_128_64 = train_fit_pq(M=128, Ks=64, X=X)\n",
    "# pq_128_128_code, pq_128_128 = train_fit_pq(M=128, Ks=128, X=X)\n",
    "\n",
    "# np.save(\"PQ/pq_128_8_code.npy\", pq_128_8_code)\n",
    "# np.save(\"PQ/pq_128_16_code.npy\", pq_128_16_code)\n",
    "# np.save(\"PQ/pq_128_32_code.npy\", pq_128_32_code)\n",
    "# np.save(\"PQ/pq_128_64_code.npy\", pq_128_64_code)\n",
    "# np.save(\"PQ/pq_128_128_code.npy\", pq_128_128_code)\n",
    "\n",
    "# pickle.dump(pq_128_8, open(\"PQ/pq_128_8.pkl\", \"wb\"))\n",
    "# pickle.dump(pq_128_16, open(\"PQ/pq_128_16.pkl\", \"wb\"))\n",
    "# pickle.dump(pq_128_32, open(\"PQ/pq_128_32.pkl\", \"wb\"))\n",
    "# pickle.dump(pq_128_64, open(\"PQ/pq_128_64.pkl\", \"wb\"))\n",
    "# pickle.dump(pq_128_128, open(\"PQ/pq_128_128.pkl\", \"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for results in PQ Search Results directory\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Define the values of M and Ks\n",
    "Ms = [8, 16, 32]\n",
    "Ks = [8, 16, 32, 64, 128]\n",
    "\n",
    "# Create directories for each PQ configuration\n",
    "for M in Ms:\n",
    "    for K in Ks:\n",
    "        dirname = \"PQ/PQ Search Results/pq_{}_{}_results\".format(M, K)\n",
    "        pathlib.Path(dirname).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preloading\n",
    "The data is preloaded from the file to prevent unnecessary preprocessing time when running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading df data....\n",
      "Loading Count Vectorizer....\n",
      "Loading X vector....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 1.3.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pq_8 objects....\n",
      "Loading pq_16 objects....\n",
      "Loading pq_32 objects....\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load data\n",
    "print(\"Loading df data....\")\n",
    "df = pd.read_json(\"job_postings_prepped.json\")\n",
    "\n",
    "# Initialize Count Vectorizer for fitting Query at runtime\n",
    "print(\"Loading Count Vectorizer....\")\n",
    "vectorizer = CountVectorizer()\n",
    "# load from vectorizer.pkl\n",
    "vectorizer = pickle.load(open(\"vectorizer.pkl\", \"rb\"))\n",
    "\n",
    "# Load the X vector from the file\n",
    "print(\"Loading X vector....\")\n",
    "X = np.load(\"PQ/X.npy\")\n",
    "\n",
    "# Load the PQ object and the X_code from the file\n",
    "print(\"Loading pq_8 objects....\")\n",
    "pq_8_8 = pickle.load(open(\"PQ/pq_8_8.pkl\", \"rb\"))\n",
    "pq_8_8_code = np.load(\"PQ/pq_8_8_code.npy\")\n",
    "\n",
    "pq_8_16 = pickle.load(open(\"PQ/pq_8_16.pkl\", \"rb\"))\n",
    "pq_8_16_code = np.load(\"PQ/pq_8_16_code.npy\")\n",
    "\n",
    "pq_8_32 = pickle.load(open(\"PQ/pq_8_32.pkl\", \"rb\"))\n",
    "pq_8_32_code = np.load(\"PQ/pq_8_32_code.npy\")\n",
    "\n",
    "pq_8_64 = pickle.load(open(\"PQ/pq_8_64.pkl\", \"rb\"))\n",
    "pq_8_64_code = np.load(\"PQ/pq_8_64_code.npy\")\n",
    "\n",
    "pq_8_128 = pickle.load(open(\"PQ/pq_8_128.pkl\", \"rb\"))\n",
    "pq_8_128_code = np.load(\"PQ/pq_8_128_code.npy\")\n",
    "\n",
    "# ===============================================================\n",
    "print(\"Loading pq_16 objects....\")\n",
    "pq_16_8 = pickle.load(open(\"PQ/pq_16_8.pkl\", \"rb\"))\n",
    "pq_16_8_code = np.load(\"PQ/pq_16_8_code.npy\")\n",
    "\n",
    "pq_16_16 = pickle.load(open(\"PQ/pq_16_16.pkl\", \"rb\"))\n",
    "pq_16_16_code = np.load(\"PQ/pq_16_16_code.npy\")\n",
    "\n",
    "pq_16_32 = pickle.load(open(\"PQ/pq_16_32.pkl\", \"rb\"))\n",
    "pq_16_32_code = np.load(\"PQ/pq_16_32_code.npy\")\n",
    "\n",
    "pq_16_64 = pickle.load(open(\"PQ/pq_16_64.pkl\", \"rb\"))\n",
    "pq_16_64_code = np.load(\"PQ/pq_16_64_code.npy\")\n",
    "\n",
    "pq_16_128 = pickle.load(open(\"PQ/pq_16_128.pkl\", \"rb\"))\n",
    "pq_16_128_code = np.load(\"PQ/pq_16_128_code.npy\")\n",
    "\n",
    "# ===============================================================\n",
    "print(\"Loading pq_32 objects....\")\n",
    "pq_32_8 = pickle.load(open(\"PQ/pq_32_8.pkl\", \"rb\"))\n",
    "pq_32_8_code = np.load(\"PQ/pq_32_8_code.npy\")\n",
    "\n",
    "pq_32_16 = pickle.load(open(\"PQ/pq_32_16.pkl\", \"rb\"))\n",
    "pq_32_16_code = np.load(\"PQ/pq_32_16_code.npy\")\n",
    "\n",
    "pq_32_32 = pickle.load(open(\"PQ/pq_32_32.pkl\", \"rb\"))\n",
    "pq_32_32_code = np.load(\"PQ/pq_32_32_code.npy\")\n",
    "\n",
    "pq_32_64 = pickle.load(open(\"PQ/pq_32_64.pkl\", \"rb\"))\n",
    "pq_32_64_code = np.load(\"PQ/pq_32_64_code.npy\")\n",
    "\n",
    "pq_32_128 = pickle.load(open(\"PQ/pq_32_128.pkl\", \"rb\"))\n",
    "pq_32_128_code = np.load(\"PQ/pq_32_128_code.npy\")\n",
    "\n",
    "# ==============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQ Code Name:  pq_8_128\n",
      "Vector Dimension:  105248\n",
      "PQ Code Vector Dimension:  8\n",
      "Original Vector Size (MB):  6377.6568603515625\n",
      "PQ Code Vector Size (MB):  0.12119293212890625\n",
      "Compression Ratio:  52624.0 x\n",
      "Space Reduction:  99.9980997263606 %\n",
      "\n",
      "PQ Code Name:  pq_16_128\n",
      "Vector Dimension:  105248\n",
      "PQ Code Vector Dimension:  16\n",
      "Original Vector Size (MB):  6377.6568603515625\n",
      "PQ Code Vector Size (MB):  0.2423858642578125\n",
      "Compression Ratio:  26312.0 x\n",
      "Space Reduction:  99.99619945272119 %\n",
      "\n",
      "PQ Code Name:  pq_32_128\n",
      "Vector Dimension:  105248\n",
      "PQ Code Vector Dimension:  32\n",
      "Original Vector Size (MB):  6377.6568603515625\n",
      "PQ Code Vector Size (MB):  0.484771728515625\n",
      "Compression Ratio:  13156.0 x\n",
      "Space Reduction:  99.99239890544239 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_space_analysis(orig_vector, pq_code_vector, name):\n",
    "    # Get the space reduction\n",
    "    orig_vector_size = orig_vector.nbytes / 1024**2\n",
    "    pq_code_vector_size = pq_code_vector.nbytes / 1024**2\n",
    "    space_reduction = (1 - pq_code_vector_size / orig_vector_size) * 100\n",
    "    print(\"PQ Code Name: \", name)\n",
    "    print(\"Vector Dimension: \", len(orig_vector[0]))\n",
    "    print(\"PQ Code Vector Dimension: \", len(pq_code_vector[0]))\n",
    "    print(\"Original Vector Size (MB): \", orig_vector_size)\n",
    "    print(\"PQ Code Vector Size (MB): \", pq_code_vector_size)\n",
    "    print(\"Compression Ratio: \", orig_vector_size / pq_code_vector_size, \"x\")\n",
    "    print(\"Space Reduction: \", space_reduction, \"%\\n\")\n",
    "    \n",
    "get_space_analysis(X, pq_8_128_code, \"pq_8_128\")\n",
    "get_space_analysis(X, pq_16_128_code, \"pq_16_128\")\n",
    "get_space_analysis(X, pq_32_128_code, \"pq_32_128\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Asymmetric Distance\n",
    "\n",
    "Runtime function to compute similarity search using asymmetric distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarity for: Hey ...\n",
      "Getting Query Results Ranking....\n",
      "Top 5 Results:\n",
      "                                                     Data  \\\n",
      "11370  Account Manager The Bachrach Group collaborati...   \n",
      "13367  IAM Ping Federate Consultant Location: Irving ...   \n",
      "6872             Manager new location. looking positions   \n",
      "1469   Performance Test Engineer Performance Tetser W...   \n",
      "1473   Multiple Open Positions Inquiry open positions...   \n",
      "\n",
      "                                                  Vector  Distance  \n",
      "11370  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  9.561901  \n",
      "13367  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  9.561901  \n",
      "6872   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  9.561901  \n",
      "1469   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  9.561901  \n",
      "1473   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  9.561901  \n",
      "Saving pq_8_128 Results to CSV....\n",
      "pq_8_128 Results Saved to CSV!\n",
      "\n",
      "===============================================================\n",
      "Total Time Taken:\n",
      "\n",
      "Start Time:  1697001996.722618\n",
      "Compute Similarity Time:  0.06349802017211914\n",
      "Get Ranking Time:  0.018140792846679688\n",
      "Save to CSV Time:  1.870171070098877\n",
      "Basic Time Taken:  1.9519267082214355\n",
      "True Computed Time Taken:  1.9518098831176758\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def compute_similarity(query_text, pq_code, pq):\n",
    "    \"\"\"\n",
    "    Compute the similarity between the query text and the pq_code\n",
    "    :param query_text: the query text\n",
    "    :param pq_code: the pq_code\n",
    "    :param pq: the pq encoder\n",
    "    :return: the similarity\n",
    "    \"\"\"\n",
    "    print(\"Computing similarity for: {}...\".format(query_text))\n",
    "    # Transform the query text to single vector\n",
    "    query_vector = vectorizer.transform([query_text]).toarray().reshape(-1).astype(np.float32)\n",
    "\n",
    "    # print(\"Query Vector Dimension: \", query_vector.shape)\n",
    "    # print(\"PQ Code Shape: \", pq_code.shape)\n",
    "    # Initialize the distance table\n",
    "    distance_table = pq.dtable(query_vector)\n",
    "    # print(\"Distance Table Shape: \", distance_table)\n",
    "    distance = distance_table.adist(pq_code)\n",
    "    end = time.time()\n",
    "    return distance, end\n",
    "\n",
    "def get_query_res_ranking(df, adist_result):\n",
    "    print(\"Getting Query Results Ranking....\")\n",
    "    # Create a new dataframe with df and append adist_result to it\n",
    "    results_df = df.copy()\n",
    "    results_df['Distance'] = adist_result\n",
    "    \n",
    "    # Sort the results by distance\n",
    "    results_df = results_df.sort_values(by=['Distance'])\n",
    "    print(\"Top 5 Results:\\n\", results_df.head())\n",
    "    end = time.time()\n",
    "    return results_df, end\n",
    "\n",
    "def save_to_csv_similarity_results(df, query, pq_setting):\n",
    "    print(\"Saving {} Results to CSV....\".format(pq_setting))\n",
    "    \n",
    "    # Parse query to strip spaces and replace spaces with underscores\n",
    "    query = query.strip().replace(\" \", \"_\")\n",
    "    \n",
    "    # Drop the vector column \n",
    "    df.drop(columns=['Vector'], inplace=True)\n",
    "    # Save the results to a csv file\n",
    "    df.to_csv(\"PQ/PQ Search Results/{}_results/{}.csv\".format(pq_setting, query), index=False)\n",
    "    end = time.time()\n",
    "    print(\"{} Results Saved to CSV!\\n\".format(pq_setting))\n",
    "    return end\n",
    "\n",
    "# # Print the top 5 results\n",
    "# def print_top_5_results(df, idx):\n",
    "#     print(\"Top 5 Results:\")\n",
    "#     print(df.iloc[idx.argsort()[:5]])\n",
    "    \n",
    "# # print all the results with the similarity score\n",
    "# def print_all_results(df, idx):\n",
    "#     print(\"All Results:\")\n",
    "#     print(df.iloc[idx.argsort()])\n",
    "    \n",
    "def search_using_query(query, pq_setting, pq_code, pq, df):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Compute the similarity\n",
    "    dist, compute_similarity_time = compute_similarity(query, pq_code, pq)\n",
    "    \n",
    "    # Get the query results ranking\n",
    "    result_df, get_rank_time = get_query_res_ranking(df, dist)\n",
    "    \n",
    "    # Save the results to a csv file\n",
    "    save_to_csv_time = save_to_csv_similarity_results(result_df, query, pq_setting)\n",
    "    \n",
    "    end = time.time()\n",
    "    # Print the time taken\n",
    "    print(\"===============================================================\")\n",
    "    print(\"Total Time Taken:\\n\")\n",
    "    print(\"Start Time: \", start)\n",
    "    print(\"Compute Similarity Time: \", compute_similarity_time - start)\n",
    "    print(\"Get Ranking Time: \", get_rank_time - compute_similarity_time)\n",
    "    print(\"Save to CSV Time: \", save_to_csv_time - get_rank_time)\n",
    "    print(\"Basic Time Taken: \", end - start)\n",
    "    print(\"True Computed Time Taken: \", (compute_similarity_time - start) + (get_rank_time - compute_similarity_time) + (save_to_csv_time - get_rank_time))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# =============================================================== Test Bench ===============================================================\n",
    "\n",
    "query = \"Hey \"\n",
    "# dist, s_t, e_t = compute_similarity(query, pq_8_128_code, pq_8_128)\n",
    "# result_df = get_query_res_ranking(df, dist)\n",
    "# save_to_csv_similarity_results(result_df, query, \"pq_8_128\")\n",
    "\n",
    "search_using_query(query, \"pq_8_128\", pq_8_128_code, pq_8_128, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Dict Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Dimension:  105248\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q1</th>\n",
       "      <td>resilient investment banker</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q2</th>\n",
       "      <td>2 years experience product manager</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q3</th>\n",
       "      <td>10 years risk analyst problem solver</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4</th>\n",
       "      <td>tax analyst for big company</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q5</th>\n",
       "      <td>software engineer for google or amazon</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Query  \\\n",
       "Q1             resilient investment banker   \n",
       "Q2      2 years experience product manager   \n",
       "Q3    10 years risk analyst problem solver   \n",
       "Q4             tax analyst for big company   \n",
       "Q5  software engineer for google or amazon   \n",
       "\n",
       "                                               Vector  \n",
       "Q1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "Q2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "Q3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "Q4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "Q5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing of Query Dict Processing, TODO: refactor into a function and leave the vector processing to the function caller\n",
    "# Import Libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Defining queries\n",
    "query_dict = {'Q1': \"resilient investment banker\", \n",
    "              'Q2': \"2 years experience product manager\", \n",
    "              'Q3': \"10 years risk analyst problem solver\", \n",
    "              'Q4': \"tax analyst for big company\", \n",
    "              'Q5': \"software engineer for google or amazon\", \n",
    "              'Q6': \"video editor for advertisements with 5 year experience\",\n",
    "              'Q7': \"full time senior head nurse position\",\n",
    "              'Q8': \"after school math and science tutor\",\n",
    "              'Q9': \"dietitian for professional atheletes\",\n",
    "              'Q10': \"costume designer and makeup artist\"}\n",
    "\n",
    "# Create a dataframe from the query dictionary\n",
    "query_df = pd.DataFrame.from_dict(query_dict, orient='index', columns=['Query'])\n",
    "\n",
    "# Count Vectorize the queries and store the vectors as float32 in the dataframe\n",
    "vectorizer = CountVectorizer()\n",
    "# Fit the df['Data'] into the vectorizer to get the same dimension\n",
    "vectorizer.fit(df['Data'])\n",
    "\n",
    "# Transform the querys into vectors\n",
    "query_df['Vector'] = [vectorizer.transform([query]).toarray()[0].astype(np.float32) for query in query_df['Query'].values]\n",
    "\n",
    "# # Create the Matrix\n",
    "# matrix = vectorizer.fit_transform(list(query_df[\"Query\"].array)).toarray()\n",
    "\n",
    "# # Add into new column in df\n",
    "# query_df[\"Vector\"] = [row.tolist() for row in matrix]\n",
    "\n",
    "# # Encode the vectors \n",
    "# # query_df['PQ_8_64'] = pq_8_64.encode(query_df['Vector'].values.tolist())\n",
    "\n",
    "print(\"Vector Dimension: \", len(query_df['Vector'][-1]))\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     df\u001b[39m.\u001b[39miloc[idx\u001b[39m.\u001b[39margsort()]\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mPQ/Results/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m query \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mResults Saved to CSV!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X31sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m query_row \u001b[39m=\u001b[39m query_df\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X31sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m idx \u001b[39m=\u001b[39m compute_similarity(query_row, pq_8_64_code, pq_8_64)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X31sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSimilarity: \u001b[39m\u001b[39m\"\u001b[39m, idx)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Working with one query\n",
    "import numpy as np\n",
    "\n",
    "# Compute Similarity between Query and the Compressed Vector\n",
    "def compute_similarity(query_row, pq_code, pq):\n",
    "    print(\"Computing Similarity for Query:\\n\", query_row['Query'])\n",
    "    query_vector = query_row['Vector']\n",
    "    distance_table = pq.dtable(query=query_vector)\n",
    "    dists = distance_table.adist(codes=pq_code)\n",
    "    return dists\n",
    "\n",
    "# Print the top 5 results\n",
    "def print_top_5_results(df, idx):\n",
    "    print(\"Top 5 Results:\")\n",
    "    print(df.iloc[idx.argsort()[:5]])\n",
    "    \n",
    "# print all the results with the similarity score\n",
    "def print_all_results(df, idx):\n",
    "    print(\"All Results:\")\n",
    "    print(df.iloc[idx.argsort()])\n",
    "    \n",
    "\n",
    "# Save as csv the results\n",
    "def save_to_csv_results(df, idx, query):\n",
    "    print(\"Saving Results to CSV....\")\n",
    "    df.iloc[idx.argsort()].to_csv(\"PQ/Results/\" + query + \".csv\", index=False)\n",
    "    print(\"Results Saved to CSV!\")\n",
    "\n",
    "query_row = query_df.iloc[0]\n",
    "idx = compute_similarity(query_row, pq_8_64_code, pq_8_64)\n",
    "print(\"Similarity: \", idx)\n",
    "\n",
    "# print_top_5_results(df, idx)\n",
    "# print_all_results(df, idx)\n",
    "\n",
    "# save_to_csv_results(df, idx, query_row['Query'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      " 10    Commercial Property Manager/Senior Property Ma...\n",
      "Name: Data, dtype: object\n",
      "Top 5 results:\n",
      " [   10 13400  4324 13332 12157]\n",
      "Top 5 Similar Data\n",
      " 10       Commercial Property Manager/Senior Property Ma...\n",
      "13401    Director Property Management Job Summary: Seek...\n",
      "4324     Commercial Broker Property Manager Commercial ...\n",
      "13333    Assistant Property Manager (CA) PURE Property ...\n",
      "12158    Assistant Property Manager Assistant Property ...\n",
      "Name: Data, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Write result into csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Performance\n",
    "With the PQ Codes we can calculate the performance of the search.\n",
    "We use the recall@k metric to calculate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb Cell 20\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m precision_score\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m n_queries \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m query_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mlen\u001b[39m(X_test), n_queries, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# print(\"Query Indices: \", query_indices)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/GitHub/Data_Analytics_Project/ProductQuantization.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m query_vec \u001b[39m=\u001b[39m X_test[\u001b[39m19\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "n_queries = 100\n",
    "query_indices = np.random.choice(len(X_test), n_queries, replace=False)\n",
    "# print(\"Query Indices: \", query_indices)\n",
    "\n",
    "query_vec = X_test[19]\n",
    "query = df['Data'][df['Vector'].apply(lambda x: np.array_equal(x, query_vec))]\n",
    "# query_vectors = X_test[query_indices].astype(np.float32)\n",
    "query_vectors = X_test[query_indices].astype(np.float32)\n",
    "print(\"Query Vectors: \", query_vectors.shape)\n",
    "\n",
    "# Compute performance metrics\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for query_vec in query_vectors:\n",
    "    indices = compute_similarity(query_vec, X_code)\n",
    "    ground_truth = df['Data'].iloc[indices[:5]].values\n",
    "    print(\"Ground Truth: \", ground_truth)\n",
    "    \n",
    "    # Calculate precision\n",
    "    top5_results = df['Data'].iloc[indices[:5]].values\n",
    "    print(\"Top 5 Results: \", top5_results)\n",
    "    p = precision_score(ground_truth, top5_results, average='micro')\n",
    "    precision.append(p)\n",
    "\n",
    "\n",
    "print(\"Precision: \", np.mean(precision))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Vector:  [0. 0. 0. ... 0. 0. 0.] (32155,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Data:  109    sunnydayzsoon StackieRobinsn BALENCIAGA You do...\n",
      "Name: Data, dtype: object\n",
      "Top 5 closest vectors:  [  4 508 247 522 697]\n",
      "Actual Data:  4      Pup_Dior_ Happy Valentines Day  You are so gor...\n",
      "508                  Little chimmy with the perfume Dior\n",
      "247    Like a Lily in the mire a beautiful flower blo...\n",
      "522    Louis Vuitton clothing has just made new Night...\n",
      "697    and I still havent gotten a Junho and Eunwoo i...\n",
      "Name: Data, dtype: object\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
